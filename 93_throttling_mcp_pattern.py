"""
Throttling MCP Pattern

This pattern controls the rate of operations to prevent system overload
and ensure fair resource usage across multiple clients.

Key Features:
- Request rate limiting
- Traffic shaping
- Burst handling
- Adaptive throttling
- Fair resource allocation
"""

from typing import TypedDict, Sequence, Annotated, List, Dict
import operator
import time
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langgraph.graph import StateGraph, START, END


# Define the state
class ThrottlingState(TypedDict):
    """State for throttling pattern"""
    messages: Annotated[Sequence[HumanMessage | SystemMessage | AIMessage], operator.add]
    service_name: str
    throttle_strategy: str  # "token_bucket", "leaky_bucket", "fixed_window", "sliding_window"
    rate_limit: int  # requests per time unit
    time_unit: str  # "second", "minute", "hour"
    burst_size: int  # max burst capacity
    current_tokens: int
    requests_allowed: int
    requests_throttled: int
    client_limits: Dict[str, int]  # per-client limits
    adaptive_enabled: bool


# Initialize LLM
llm = ChatOpenAI(model="gpt-4", temperature=0)


# Throttle Manager
def throttle_manager(state: ThrottlingState) -> ThrottlingState:
    """Manages throttling configuration and strategy"""
    service_name = state.get("service_name", "")
    throttle_strategy = state.get("throttle_strategy", "token_bucket")
    rate_limit = state.get("rate_limit", 100)
    time_unit = state.get("time_unit", "second")
    burst_size = state.get("burst_size", 10)
    
    system_message = SystemMessage(content="""You are a throttle manager.
    Configure and manage rate limiting strategies.""")
    
    user_message = HumanMessage(content=f"""Configure throttling:

Service: {service_name}
Strategy: {throttle_strategy}
Rate Limit: {rate_limit}/{time_unit}
Burst Size: {burst_size}

Set up throttling mechanism.""")
    
    response = llm.invoke([system_message, user_message])
    
    # Initialize tokens based on strategy
    current_tokens = burst_size if throttle_strategy == "token_bucket" else 0
    
    report = f"""
    ðŸš¦ Throttle Management:
    
    Configuration:
    â€¢ Service: {service_name}
    â€¢ Strategy: {throttle_strategy.upper()}
    â€¢ Rate Limit: {rate_limit} requests/{time_unit}
    â€¢ Burst Capacity: {burst_size}
    â€¢ Current Tokens: {current_tokens}
    
    Throttling Strategies:
    
    1. Token Bucket:
       â€¢ Tokens added at fixed rate
       â€¢ Burst traffic supported
       â€¢ Tokens consumed per request
       â€¢ Most flexible strategy
       â€¢ Example: AWS API Gateway
    
    2. Leaky Bucket:
       â€¢ Fixed output rate
       â€¢ Queue requests
       â€¢ Smooth traffic flow
       â€¢ No burst support
       â€¢ Example: Network traffic shaping
    
    3. Fixed Window:
       â€¢ Counter resets at intervals
       â€¢ Simple implementation
       â€¢ Burst at boundaries
       â€¢ Example: GitHub API (5000/hour)
    
    4. Sliding Window:
       â€¢ Rolling time window
       â€¢ More accurate than fixed
       â€¢ Higher computation cost
       â€¢ Example: Twitter API v2
    
    When to Use Each:
    
    Token Bucket:
    âœ“ Need burst handling
    âœ“ Variable request sizes
    âœ“ Most APIs and services
    âœ“ Cloud service throttling
    
    Leaky Bucket:
    âœ“ Smooth output required
    âœ“ Network traffic control
    âœ“ Video streaming
    âœ“ Queue-based systems
    
    Fixed Window:
    âœ“ Simple requirements
    âœ“ Hour/day quotas
    âœ“ Low traffic services
    âœ“ Billing periods
    
    Sliding Window:
    âœ“ Precise rate limiting
    âœ“ High traffic services
    âœ“ SLA enforcement
    âœ“ DDoS protection
    
    Industry Examples:
    
    AWS:
    â€¢ API Gateway: Token bucket
    â€¢ DynamoDB: Adaptive throttling
    â€¢ Lambda: Concurrent execution limits
    â€¢ CloudWatch: API throttling
    
    Google Cloud:
    â€¢ Rate limiting per 100 seconds
    â€¢ Per-user rate limits
    â€¢ Burst quotas
    â€¢ Daily quotas
    
    GitHub API:
    â€¢ 5,000 requests/hour (authenticated)
    â€¢ 60 requests/hour (unauthenticated)
    â€¢ Separate GraphQL limits
    â€¢ Conditional requests optimization
    
    Twitter API:
    â€¢ App rate limit
    â€¢ User rate limit
    â€¢ Endpoint-specific limits
    â€¢ 15-minute windows
    """
    
    return {
        "messages": [AIMessage(content=f"ðŸš¦ Throttle Manager:\n{response.content}\n{report}")],
        "current_tokens": current_tokens
    }


# Rate Limiter
def rate_limiter(state: ThrottlingState) -> ThrottlingState:
    """Enforces rate limits on incoming requests"""
    throttle_strategy = state.get("throttle_strategy", "token_bucket")
    rate_limit = state.get("rate_limit", 100)
    current_tokens = state.get("current_tokens", 0)
    burst_size = state.get("burst_size", 10)
    
    system_message = SystemMessage(content="""You are a rate limiter.
    Enforce rate limits and manage request throughput.""")
    
    # Simulate incoming requests
    incoming_requests = 15
    
    user_message = HumanMessage(content=f"""Process requests with rate limiting:

Strategy: {throttle_strategy}
Rate Limit: {rate_limit}
Current Tokens: {current_tokens}
Burst Size: {burst_size}
Incoming Requests: {incoming_requests}

Apply rate limiting.""")
    
    response = llm.invoke([system_message, user_message])
    
    # Simulate rate limiting
    requests_allowed = min(incoming_requests, current_tokens)
    requests_throttled = incoming_requests - requests_allowed
    current_tokens = max(0, current_tokens - requests_allowed)
    
    report = f"""
    âš¡ Rate Limiting:
    
    Request Processing:
    â€¢ Incoming Requests: {incoming_requests}
    â€¢ Requests Allowed: {requests_allowed}
    â€¢ Requests Throttled: {requests_throttled}
    â€¢ Remaining Tokens: {current_tokens}
    â€¢ Throttle Rate: {(requests_throttled/incoming_requests*100):.1f}%
    
    Rate Limiting Algorithms:
    
    Token Bucket Implementation:
    ```
    class TokenBucket:
        def __init__(self, rate, capacity):
            self.rate = rate          # tokens/second
            self.capacity = capacity  # max tokens
            self.tokens = capacity
            self.last_update = time()
        
        def allow_request(self, tokens=1):
            self.refill()
            if self.tokens >= tokens:
                self.tokens -= tokens
                return True
            return False
        
        def refill(self):
            now = time()
            elapsed = now - self.last_update
            new_tokens = elapsed * self.rate
            self.tokens = min(
                self.capacity,
                self.tokens + new_tokens
            )
            self.last_update = now
    ```
    
    Sliding Window Log:
    ```
    class SlidingWindowLog:
        def __init__(self, rate, window):
            self.rate = rate
            self.window = window  # seconds
            self.requests = []
        
        def allow_request(self):
            now = time()
            cutoff = now - self.window
            
            # Remove old requests
            self.requests = [
                t for t in self.requests
                if t > cutoff
            ]
            
            if len(self.requests) < self.rate:
                self.requests.append(now)
                return True
            return False
    ```
    
    Fixed Window Counter:
    ```
    class FixedWindow:
        def __init__(self, rate, window):
            self.rate = rate
            self.window = window
            self.counter = 0
            self.window_start = time()
        
        def allow_request(self):
            now = time()
            
            # Reset window if expired
            if now - self.window_start >= self.window:
                self.counter = 0
                self.window_start = now
            
            if self.counter < self.rate:
                self.counter += 1
                return True
            return False
    ```
    
    Response Headers:
    â€¢ X-RateLimit-Limit: {rate_limit}
    â€¢ X-RateLimit-Remaining: {current_tokens}
    â€¢ X-RateLimit-Reset: {int(time.time() + 60)}
    â€¢ Retry-After: {60 if requests_throttled > 0 else 0}
    
    HTTP Status Codes:
    â€¢ 200 OK: Request allowed
    â€¢ 429 Too Many Requests: Throttled
    â€¢ 503 Service Unavailable: Overload
    
    Client Best Practices:
    â€¢ Respect rate limit headers
    â€¢ Implement exponential backoff
    â€¢ Use Retry-After header
    â€¢ Batch requests when possible
    â€¢ Cache responses
    â€¢ Use webhooks vs polling
    """
    
    return {
        "messages": [AIMessage(content=f"âš¡ Rate Limiter:\n{response.content}\n{report}")],
        "current_tokens": current_tokens,
        "requests_allowed": requests_allowed,
        "requests_throttled": requests_throttled
    }


# Adaptive Controller
def adaptive_controller(state: ThrottlingState) -> ThrottlingState:
    """Adjusts throttling based on system load"""
    adaptive_enabled = state.get("adaptive_enabled", True)
    rate_limit = state.get("rate_limit", 100)
    requests_throttled = state.get("requests_throttled", 0)
    
    system_message = SystemMessage(content="""You are an adaptive throttling controller.
    Adjust rate limits dynamically based on system conditions.""")
    
    user_message = HumanMessage(content=f"""Adaptive throttling:

Enabled: {adaptive_enabled}
Base Rate: {rate_limit}
Throttled: {requests_throttled}

Adjust throttling based on load.""")
    
    response = llm.invoke([system_message, user_message])
    
    # Simulate adaptive adjustment
    if adaptive_enabled and requests_throttled > 0:
        adjustment = "Decrease rate (high load detected)"
        new_rate = int(rate_limit * 0.8)
    else:
        adjustment = "Maintain rate (system healthy)"
        new_rate = rate_limit
    
    report = f"""
    ðŸŽ¯ Adaptive Throttling:
    
    System Analysis:
    â€¢ Adaptive Mode: {'Enabled âœ…' if adaptive_enabled else 'Disabled'}
    â€¢ Current Rate: {rate_limit}
    â€¢ Suggested Rate: {new_rate}
    â€¢ Adjustment: {adjustment}
    â€¢ Throttled Requests: {requests_throttled}
    
    Adaptive Strategies:
    
    1. Load-Based:
       â€¢ Monitor CPU/Memory
       â€¢ Adjust limits dynamically
       â€¢ Prevent overload
       â€¢ Gradual rate changes
       â€¢ DynamoDB approach
    
    2. Response Time:
       â€¢ Track latency metrics
       â€¢ Reduce rate if slow
       â€¢ Maintain SLA targets
       â€¢ P95/P99 monitoring
    
    3. Error Rate:
       â€¢ Monitor error percentage
       â€¢ Throttle on high errors
       â€¢ Circuit breaker integration
       â€¢ Automatic recovery
    
    4. Queue Depth:
       â€¢ Track pending requests
       â€¢ Limit queue growth
       â€¢ Prevent memory issues
       â€¢ Backpressure signal
    
    Adaptive Algorithms:
    
    AIMD (Additive Increase, Multiplicative Decrease):
    â€¢ Increase: rate = rate + 1 (gradual)
    â€¢ Decrease: rate = rate * 0.5 (aggressive)
    â€¢ TCP congestion control style
    â€¢ Stable convergence
    
    PID Controller:
    â€¢ Proportional-Integral-Derivative
    â€¢ Smooth rate adjustments
    â€¢ Minimize oscillation
    â€¢ Engineering control theory
    
    Machine Learning:
    â€¢ Predict optimal rate
    â€¢ Historical pattern analysis
    â€¢ Anomaly detection
    â€¢ Seasonal adjustments
    
    Implementation Example:
    ```python
    class AdaptiveThrottle:
        def __init__(self, base_rate):
            self.base_rate = base_rate
            self.current_rate = base_rate
            self.min_rate = base_rate * 0.1
            self.max_rate = base_rate * 2.0
        
        def adjust(self, metrics):
            cpu = metrics['cpu_percent']
            latency = metrics['p95_latency_ms']
            error_rate = metrics['error_rate']
            
            # Decrease if overloaded
            if cpu > 80 or latency > 1000 or error_rate > 0.05:
                self.current_rate *= 0.9
            # Increase if underutilized
            elif cpu < 50 and latency < 100 and error_rate < 0.01:
                self.current_rate *= 1.1
            
            # Clamp to limits
            self.current_rate = max(
                self.min_rate,
                min(self.max_rate, self.current_rate)
            )
            
            return self.current_rate
    ```
    
    Monitoring Metrics:
    â€¢ Throttle rate
    â€¢ Request acceptance rate
    â€¢ System resource utilization
    â€¢ Response time distribution
    â€¢ Error rate trends
    â€¢ Queue length
    
    Auto-Scaling Integration:
    â€¢ Scale out when throttling high
    â€¢ Scale in when capacity excess
    â€¢ Coordinate with load balancer
    â€¢ Health check integration
    """
    
    return {
        "messages": [AIMessage(content=f"ðŸŽ¯ Adaptive Controller:\n{response.content}\n{report}")]
    }


# Throttle Monitor
def throttle_monitor(state: ThrottlingState) -> ThrottlingState:
    """Monitors throttling metrics and provides insights"""
    service_name = state.get("service_name", "")
    throttle_strategy = state.get("throttle_strategy", "")
    rate_limit = state.get("rate_limit", 0)
    requests_allowed = state.get("requests_allowed", 0)
    requests_throttled = state.get("requests_throttled", 0)
    client_limits = state.get("client_limits", {})
    
    total_requests = requests_allowed + requests_throttled
    throttle_percentage = (requests_throttled / total_requests * 100) if total_requests > 0 else 0
    
    summary = f"""
    ðŸ“Š THROTTLING COMPLETE
    
    Service Status:
    â€¢ Service: {service_name}
    â€¢ Strategy: {throttle_strategy.upper()}
    â€¢ Rate Limit: {rate_limit}
    â€¢ Requests Allowed: {requests_allowed}
    â€¢ Requests Throttled: {requests_throttled}
    â€¢ Throttle Rate: {throttle_percentage:.1f}%
    
    Throttling Pattern Process:
    1. Throttle Manager â†’ Configure strategy
    2. Rate Limiter â†’ Enforce limits
    3. Adaptive Controller â†’ Dynamic adjustments
    4. Monitor â†’ Track performance
    
    Common Throttling Use Cases:
    
    API Rate Limiting:
    â€¢ Prevent abuse
    â€¢ Fair usage enforcement
    â€¢ Cost control
    â€¢ Infrastructure protection
    â€¢ SLA compliance
    
    DDoS Protection:
    â€¢ Block attack traffic
    â€¢ Preserve service availability
    â€¢ Geographic rate limits
    â€¢ IP-based throttling
    â€¢ Challenge-response (CAPTCHA)
    
    Resource Protection:
    â€¢ Database connection limits
    â€¢ CPU/Memory protection
    â€¢ Disk I/O throttling
    â€¢ Network bandwidth control
    â€¢ Thread pool limits
    
    Cost Optimization:
    â€¢ Cloud API call limits
    â€¢ Third-party API costs
    â€¢ Bandwidth costs
    â€¢ Compute costs
    â€¢ Storage operations
    
    Multi-Tier Throttling:
    
    Global Limits:
    â€¢ Overall service capacity
    â€¢ Infrastructure limits
    â€¢ Total throughput
    
    Per-User Limits:
    â€¢ Fair resource sharing
    â€¢ Prevent single user monopoly
    â€¢ Tier-based limits (free/paid)
    
    Per-Endpoint Limits:
    â€¢ Expensive operations
    â€¢ Different resource costs
    â€¢ Granular control
    
    Geographic Limits:
    â€¢ Region-specific capacity
    â€¢ Compliance requirements
    â€¢ Network proximity
    
    Real-World Examples:
    
    Stripe API:
    â€¢ Default: 100 req/sec
    â€¢ Burst: Up to 150 req/sec
    â€¢ Per-endpoint limits
    â€¢ Webhook retry with backoff
    
    AWS Lambda:
    â€¢ Concurrent executions: 1000 (default)
    â€¢ Burst capacity: 3000
    â€¢ Per-region limits
    â€¢ Reserved concurrency
    
    Cloudflare:
    â€¢ 1200 req/5min (Free)
    â€¢ 2400 req/5min (Pro)
    â€¢ Enterprise: Custom
    â€¢ Rate Limiting Rules
    
    Redis:
    â€¢ Command throttling
    â€¢ Slow log tracking
    â€¢ Client output buffer limits
    â€¢ Memory-based eviction
    
    Best Practices:
    
    Design:
    â€¢ Choose appropriate strategy
    â€¢ Set reasonable defaults
    â€¢ Document limits clearly
    â€¢ Provide quota endpoints
    â€¢ Version your limits
    
    Implementation:
    â€¢ Use proven libraries
    â€¢ Distributed rate limiting (Redis)
    â€¢ Atomic operations
    â€¢ Accurate timestamps
    â€¢ Handle clock skew
    
    Client Communication:
    â€¢ Clear error messages
    â€¢ Rate limit headers
    â€¢ Retry-After header
    â€¢ Documentation
    â€¢ Status page
    
    Monitoring:
    â€¢ Track throttle rates
    â€¢ Alert on high throttling
    â€¢ Client-specific metrics
    â€¢ Trend analysis
    â€¢ Capacity planning
    
    Testing:
    â€¢ Load testing with limits
    â€¢ Burst traffic scenarios
    â€¢ Client retry behavior
    â€¢ Distributed coordination
    â€¢ Clock edge cases
    
    Key Insight:
    Throttling protects your service from overload while
    ensuring fair resource allocation. Choose the right
    strategy for your use case and monitor continuously.
    """
    
    return {
        "messages": [AIMessage(content=f"ðŸ“Š Throttle Monitor:\n{summary}")]
    }


# Build the graph
def build_throttling_graph():
    """Build the throttling pattern graph"""
    workflow = StateGraph(ThrottlingState)
    
    workflow.add_node("throttle_mgr", throttle_manager)
    workflow.add_node("rate_limiter", rate_limiter)
    workflow.add_node("adaptive", adaptive_controller)
    workflow.add_node("monitor", throttle_monitor)
    
    workflow.add_edge(START, "throttle_mgr")
    workflow.add_edge("throttle_mgr", "rate_limiter")
    workflow.add_edge("rate_limiter", "adaptive")
    workflow.add_edge("adaptive", "monitor")
    workflow.add_edge("monitor", END)
    
    return workflow.compile()


# Example usage
if __name__ == "__main__":
    graph = build_throttling_graph()
    
    print("=== Throttling MCP Pattern ===\n")
    
    # Test Case: API rate limiting with token bucket
    print("\n" + "="*70)
    print("TEST CASE: API Rate Limiting")
    print("="*70)
    
    state = {
        "messages": [],
        "service_name": "user_api",
        "throttle_strategy": "token_bucket",
        "rate_limit": 100,
        "time_unit": "second",
        "burst_size": 10,
        "current_tokens": 0,
        "requests_allowed": 0,
        "requests_throttled": 0,
        "client_limits": {"client_1": 50, "client_2": 30},
        "adaptive_enabled": True
    }
    
    result = graph.invoke(state)
    
    for msg in result["messages"]:
        print(f"\n{msg.content}")
        print("-" * 70)
    
    print(f"\nRequests Allowed: {result.get('requests_allowed', 0)}")
    print(f"Requests Throttled: {result.get('requests_throttled', 0)}")
    total = result.get('requests_allowed', 0) + result.get('requests_throttled', 0)
    if total > 0:
        print(f"Throttle Rate: {(result.get('requests_throttled', 0)/total*100):.1f}%")
