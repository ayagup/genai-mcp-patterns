"""
Logging MCP Pattern

This pattern provides structured logging capabilities for distributed systems,
collecting, formatting, and analyzing log data from multiple sources.

Key Features:
- Structured logging with levels
- Log aggregation from multiple sources
- Log formatting and enrichment
- Log analysis and search
- Correlation and context tracking
"""

from typing import TypedDict, Sequence, Annotated, List, Dict
import operator
import time
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langgraph.graph import StateGraph, START, END


# Define the state
class LoggingState(TypedDict):
    """State for logging pattern"""
    messages: Annotated[Sequence[HumanMessage | SystemMessage | AIMessage], operator.add]
    log_level: str  # "DEBUG", "INFO", "WARN", "ERROR", "FATAL"
    log_format: str  # "JSON", "TEXT", "STRUCTURED"
    log_entries: List[Dict]  # [{timestamp, level, message, context, source}]
    sources: List[str]  # List of log sources
    total_logs: int
    error_count: int
    warning_count: int
    correlation_ids: Dict[str, List[int]]  # correlation_id -> log indices


# Initialize LLM
llm = ChatOpenAI(model="gpt-4", temperature=0)


# Log Collector
def log_collector(state: LoggingState) -> LoggingState:
    """Collects logs from multiple sources"""
    sources = state.get("sources", ["app-server", "database", "cache"])
    log_level = state.get("log_level", "INFO")
    
    system_message = SystemMessage(content="""You are a log collector.
    Collect log entries from multiple sources in a distributed system.""")
    
    user_message = HumanMessage(content=f"""Collect logs:

Sources: {', '.join(sources)}
Minimum Level: {log_level}

Gather log entries from all sources.""")
    
    response = llm.invoke([system_message, user_message])
    
    # Simulate log collection
    import uuid
    log_entries = [
        {
            "timestamp": int(time.time() * 1000),
            "level": "INFO",
            "message": "Application started successfully",
            "context": {"service": "app-server", "version": "1.2.3"},
            "source": "app-server",
            "correlation_id": str(uuid.uuid4())
        },
        {
            "timestamp": int(time.time() * 1000) + 100,
            "level": "DEBUG",
            "message": "Database connection established",
            "context": {"service": "database", "pool_size": 10},
            "source": "database",
            "correlation_id": str(uuid.uuid4())
        },
        {
            "timestamp": int(time.time() * 1000) + 200,
            "level": "WARN",
            "message": "Cache miss for key: user_123",
            "context": {"service": "cache", "key": "user_123"},
            "source": "cache",
            "correlation_id": str(uuid.uuid4())
        },
        {
            "timestamp": int(time.time() * 1000) + 300,
            "level": "ERROR",
            "message": "Failed to connect to external API",
            "context": {"service": "app-server", "api": "payment-gateway", "retry": 3},
            "source": "app-server",
            "correlation_id": str(uuid.uuid4())
        },
        {
            "timestamp": int(time.time() * 1000) + 400,
            "level": "INFO",
            "message": "Request processed successfully",
            "context": {"service": "app-server", "duration_ms": 45, "status": 200},
            "source": "app-server",
            "correlation_id": str(uuid.uuid4())
        }
    ]
    
    total_logs = len(log_entries)
    error_count = sum(1 for log in log_entries if log["level"] == "ERROR")
    warning_count = sum(1 for log in log_entries if log["level"] == "WARN")
    
    report = f"""
    ðŸ“ Log Collection:
    
    Collection Summary:
    â€¢ Sources: {len(sources)}
    â€¢ Total Logs: {total_logs}
    â€¢ Errors: {error_count}
    â€¢ Warnings: {warning_count}
    â€¢ Format: Structured JSON
    
    Log Levels (Severity):
    
    TRACE/DEBUG:
    â€¢ Detailed diagnostic information
    â€¢ Development and troubleshooting
    â€¢ High volume
    â€¢ Usually disabled in production
    
    INFO:
    â€¢ General informational messages
    â€¢ Application lifecycle events
    â€¢ Normal operations
    â€¢ Audit trail
    
    WARN:
    â€¢ Warning messages
    â€¢ Potential issues
    â€¢ Degraded functionality
    â€¢ Requires attention
    
    ERROR:
    â€¢ Error conditions
    â€¢ Failed operations
    â€¢ Exceptions caught
    â€¢ Requires investigation
    
    FATAL/CRITICAL:
    â€¢ Severe errors
    â€¢ Application crash
    â€¢ Data corruption
    â€¢ Immediate action required
    
    Structured Logging Best Practices:
    
    JSON Format:
    ```json
    {{
      "timestamp": "2024-01-01T12:00:00Z",
      "level": "ERROR",
      "message": "Database connection failed",
      "context": {{
        "service": "user-service",
        "database": "postgres",
        "error": "connection timeout",
        "retry_count": 3
      }},
      "trace_id": "abc-123",
      "span_id": "xyz-789",
      "source": "user-service-pod-1"
    }}
    ```
    
    Python Structured Logging:
    ```python
    import structlog
    
    logger = structlog.get_logger()
    
    logger.info(
        "user_login",
        user_id="123",
        ip_address="192.168.1.1",
        success=True,
        duration_ms=45
    )
    ```
    
    Log Aggregation Tools:
    
    ELK Stack:
    â€¢ Elasticsearch: Search and analytics
    â€¢ Logstash: Log processing pipeline
    â€¢ Kibana: Visualization and dashboards
    
    Splunk:
    â€¢ Enterprise log management
    â€¢ Machine learning analytics
    â€¢ Security monitoring
    â€¢ Compliance reporting
    
    Datadog:
    â€¢ Cloud-native logging
    â€¢ APM integration
    â€¢ Real-time analytics
    â€¢ Alerting
    
    Loki (Grafana):
    â€¢ Prometheus-inspired
    â€¢ Label-based indexing
    â€¢ Cost-effective
    â€¢ Kubernetes-native
    
    CloudWatch Logs:
    â€¢ AWS native
    â€¢ Insights queries
    â€¢ Metric filters
    â€¢ Log groups and streams
    
    Log Collection Methods:
    
    Agent-Based:
    â€¢ Filebeat, Fluentd, Logstash
    â€¢ Tail log files
    â€¢ Forward to aggregator
    â€¢ Low application overhead
    
    Library-Based:
    â€¢ Direct logging to service
    â€¢ Application integration
    â€¢ Structured from source
    â€¢ Network dependency
    
    Sidecar Pattern:
    â€¢ Container per pod
    â€¢ Log extraction
    â€¢ Kubernetes common
    â€¢ Resource overhead
    
    Log Enrichment:
    â€¢ Add metadata (host, pod, region)
    â€¢ Correlation IDs
    â€¢ User context
    â€¢ Environment info
    â€¢ Timestamps normalization
    """
    
    return {
        "messages": [AIMessage(content=f"ðŸ“ Log Collector:\n{response.content}\n{report}")],
        "log_entries": log_entries,
        "total_logs": total_logs,
        "error_count": error_count,
        "warning_count": warning_count
    }


# Log Formatter
def log_formatter(state: LoggingState) -> LoggingState:
    """Formats and enriches log entries"""
    log_entries = state.get("log_entries", [])
    log_format = state.get("log_format", "JSON")
    
    system_message = SystemMessage(content="""You are a log formatter.
    Format and enrich log entries for consistency and searchability.""")
    
    user_message = HumanMessage(content=f"""Format logs:

Total Entries: {len(log_entries)}
Format: {log_format}

Standardize and enrich log data.""")
    
    response = llm.invoke([system_message, user_message])
    
    report = f"""
    ðŸŽ¨ Log Formatting:
    
    Formatting Applied:
    â€¢ Format: {log_format}
    â€¢ Entries Processed: {len(log_entries)}
    â€¢ Timestamp: ISO 8601
    â€¢ Enrichment: Added metadata
    
    Log Format Standards:
    
    Common Log Format (CLF):
    ```
    127.0.0.1 - - [01/Jan/2024:12:00:00 +0000] "GET /api/users HTTP/1.1" 200 1234
    ```
    
    JSON Format:
    ```json
    {{
      "@timestamp": "2024-01-01T12:00:00.000Z",
      "level": "INFO",
      "logger": "com.example.UserService",
      "message": "User login successful",
      "thread": "http-nio-8080-exec-1",
      "context": {{
        "user_id": "123",
        "session_id": "abc-xyz",
        "ip": "192.168.1.1"
      }}
    }}
    ```
    
    Logfmt:
    ```
    ts=2024-01-01T12:00:00Z level=info msg="User login" user_id=123 ip=192.168.1.1
    ```
    
    CEF (Common Event Format):
    ```
    CEF:0|Vendor|Product|Version|EventID|Name|Severity|Extension
    ```
    
    Field Standardization:
    
    Timestamp:
    â€¢ ISO 8601 format
    â€¢ UTC timezone
    â€¢ Millisecond precision
    â€¢ Consistent parsing
    
    Level Mapping:
    â€¢ TRACE â†’ 0
    â€¢ DEBUG â†’ 1
    â€¢ INFO â†’ 2
    â€¢ WARN â†’ 3
    â€¢ ERROR â†’ 4
    â€¢ FATAL â†’ 5
    
    Contextual Fields:
    â€¢ user_id: User identifier
    â€¢ session_id: Session tracking
    â€¢ request_id: Request correlation
    â€¢ trace_id: Distributed tracing
    â€¢ span_id: Trace span
    â€¢ service: Service name
    â€¢ environment: prod/staging/dev
    â€¢ version: Application version
    â€¢ host: Hostname/IP
    â€¢ pod: Kubernetes pod name
    
    Log Enrichment Techniques:
    
    Automatic:
    â€¢ Timestamp injection
    â€¢ Hostname/IP addition
    â€¢ Process ID
    â€¢ Thread ID
    â€¢ Stack trace (errors)
    
    Contextual:
    â€¢ Request headers
    â€¢ User information
    â€¢ Session data
    â€¢ Correlation IDs
    â€¢ Business context
    
    Derived:
    â€¢ Geolocation from IP
    â€¢ User agent parsing
    â€¢ URL parsing
    â€¢ Duration calculation
    â€¢ Status categorization
    
    Performance Considerations:
    
    Async Logging:
    â€¢ Non-blocking I/O
    â€¢ Background threads
    â€¢ Ring buffer
    â€¢ Batching
    
    Sampling:
    â€¢ Log only N% of requests
    â€¢ Adaptive sampling
    â€¢ Priority-based
    â€¢ Error always logged
    
    Filtering:
    â€¢ Level-based
    â€¢ Source-based
    â€¢ Pattern-based
    â€¢ Rate limiting
    """
    
    return {
        "messages": [AIMessage(content=f"ðŸŽ¨ Log Formatter:\n{response.content}\n{report}")]
    }


# Log Aggregator
def log_aggregator(state: LoggingState) -> LoggingState:
    """Aggregates and correlates log entries"""
    log_entries = state.get("log_entries", [])
    
    system_message = SystemMessage(content="""You are a log aggregator.
    Aggregate logs and identify correlated events.""")
    
    user_message = HumanMessage(content=f"""Aggregate logs:

Total Entries: {len(log_entries)}

Group and correlate related log entries.""")
    
    response = llm.invoke([system_message, user_message])
    
    # Build correlation map
    correlation_ids = {}
    for idx, log in enumerate(log_entries):
        corr_id = log.get("correlation_id", "unknown")
        if corr_id not in correlation_ids:
            correlation_ids[corr_id] = []
        correlation_ids[corr_id].append(idx)
    
    report = f"""
    ðŸ”— Log Aggregation:
    
    Aggregation Results:
    â€¢ Total Entries: {len(log_entries)}
    â€¢ Unique Correlations: {len(correlation_ids)}
    â€¢ Sources: {len(set(log.get('source', '') for log in log_entries))}
    
    Log Aggregation Patterns:
    
    Time-Based:
    â€¢ Group by time windows
    â€¢ 1-minute, 5-minute intervals
    â€¢ Trending analysis
    â€¢ Rate calculation
    
    Source-Based:
    â€¢ Group by service
    â€¢ Group by host
    â€¢ Group by environment
    â€¢ Cross-service view
    
    Correlation-Based:
    â€¢ Request ID tracking
    â€¢ Trace ID grouping
    â€¢ Session grouping
    â€¢ Transaction flows
    
    Level-Based:
    â€¢ Errors only
    â€¢ Warnings and above
    â€¢ Debug excluded
    â€¢ Severity filtering
    
    Correlation Techniques:
    
    Request Tracing:
    ```python
    import uuid
    
    # Generate correlation ID
    correlation_id = str(uuid.uuid4())
    
    # Add to all logs in request
    logger.info("Processing request", 
                correlation_id=correlation_id)
    
    # Pass to downstream services
    headers = {{'X-Correlation-ID': correlation_id}}
    ```
    
    Distributed Tracing:
    â€¢ OpenTelemetry
    â€¢ Jaeger
    â€¢ Zipkin
    â€¢ AWS X-Ray
    â€¢ Google Cloud Trace
    
    Log Aggregation Architecture:
    
    Centralized:
    â€¢ Single aggregation point
    â€¢ Simple architecture
    â€¢ Single point of failure
    â€¢ Scalability limit
    
    Distributed:
    â€¢ Multiple aggregators
    â€¢ Regional deployment
    â€¢ High availability
    â€¢ Complex coordination
    
    Hierarchical:
    â€¢ Local aggregation
    â€¢ Regional roll-up
    â€¢ Global view
    â€¢ Reduces network traffic
    
    Query Patterns:
    
    Full-Text Search:
    ```
    message:"database connection failed"
    ```
    
    Field Search:
    ```
    level:ERROR AND service:user-api
    ```
    
    Time Range:
    ```
    @timestamp:[2024-01-01 TO 2024-01-02]
    ```
    
    Aggregations:
    ```
    COUNT(*) GROUP BY level
    AVG(duration_ms) GROUP BY endpoint
    ```
    
    Best Practices:
    
    Retention:
    â€¢ Hot: 7-30 days (fast query)
    â€¢ Warm: 30-90 days (slower)
    â€¢ Cold: 90+ days (archive)
    â€¢ Compliance-based retention
    
    Indexing:
    â€¢ Index important fields
    â€¢ Full-text search fields
    â€¢ Time-series optimization
    â€¢ Balance query speed vs storage
    
    Security:
    â€¢ Encrypt in transit
    â€¢ Encrypt at rest
    â€¢ Access control
    â€¢ PII masking/redaction
    â€¢ Audit log access
    """
    
    return {
        "messages": [AIMessage(content=f"ðŸ”— Log Aggregator:\n{response.content}\n{report}")],
        "correlation_ids": correlation_ids
    }


# Log Analyzer
def log_analyzer(state: LoggingState) -> LoggingState:
    """Analyzes logs for patterns and insights"""
    log_entries = state.get("log_entries", [])
    error_count = state.get("error_count", 0)
    warning_count = state.get("warning_count", 0)
    total_logs = state.get("total_logs", 0)
    correlation_ids = state.get("correlation_ids", {})
    
    error_rate = (error_count / total_logs * 100) if total_logs > 0 else 0
    
    summary = f"""
    ðŸ“Š LOGGING COMPLETE
    
    Log Analysis Summary:
    â€¢ Total Logs: {total_logs}
    â€¢ Errors: {error_count} ({error_rate:.1f}%)
    â€¢ Warnings: {warning_count}
    â€¢ Correlations: {len(correlation_ids)}
    â€¢ Sources: {len(state.get('sources', []))}
    
    Logging Pattern Process:
    1. Log Collector â†’ Gather logs from sources
    2. Log Formatter â†’ Standardize and enrich
    3. Log Aggregator â†’ Correlate related logs
    4. Log Analyzer â†’ Extract insights
    
    Log Analysis Techniques:
    
    Pattern Detection:
    â€¢ Error patterns
    â€¢ Anomaly detection
    â€¢ Trend analysis
    â€¢ Seasonality
    â€¢ Correlation discovery
    
    Machine Learning:
    â€¢ Log clustering
    â€¢ Anomaly detection
    â€¢ Failure prediction
    â€¢ Root cause analysis
    â€¢ Auto-categorization
    
    Statistical Analysis:
    â€¢ Error rate trends
    â€¢ Response time distribution
    â€¢ Request volume patterns
    â€¢ Resource utilization
    â€¢ SLA compliance
    
    Real-World Use Cases:
    
    Debugging:
    â€¢ Trace request flow
    â€¢ Identify error source
    â€¢ Reproduce issues
    â€¢ Performance bottlenecks
    
    Security:
    â€¢ Failed login attempts
    â€¢ Suspicious patterns
    â€¢ Data access audit
    â€¢ Compliance reporting
    
    Performance:
    â€¢ Slow queries
    â€¢ High latency endpoints
    â€¢ Resource contention
    â€¢ Optimization opportunities
    
    Business Intelligence:
    â€¢ User behavior
    â€¢ Feature usage
    â€¢ Conversion funnels
    â€¢ A/B test results
    
    Log Analysis Tools:
    
    Elasticsearch Queries:
    ```json
    {{
      "query": {{
        "bool": {{
          "must": [
            {{"match": {{"level": "ERROR"}}}},
            {{"range": {{"@timestamp": {{"gte": "now-1h"}}}}}}
          ]
        }}
      }},
      "aggs": {{
        "errors_by_service": {{
          "terms": {{"field": "service.keyword"}}
        }}
      }}
    }}
    ```
    
    Splunk SPL:
    ```
    index=production level=ERROR
    | stats count by service, error_type
    | where count > 10
    ```
    
    CloudWatch Insights:
    ```
    fields @timestamp, level, message
    | filter level = "ERROR"
    | stats count() by service
    | sort count desc
    ```
    
    Key Metrics to Track:
    
    Availability:
    â€¢ Error rate
    â€¢ Success rate
    â€¢ Uptime percentage
    
    Performance:
    â€¢ Response time (p50, p95, p99)
    â€¢ Throughput (requests/sec)
    â€¢ Resource usage
    
    Quality:
    â€¢ Error types distribution
    â€¢ Warning trends
    â€¢ Exception patterns
    
    Business:
    â€¢ User actions
    â€¢ Transaction volume
    â€¢ Conversion rates
    
    Best Practices:
    
    Log What Matters:
    â€¢ Business events
    â€¢ Errors and exceptions
    â€¢ Performance metrics
    â€¢ Security events
    â€¢ State changes
    
    Don't Log:
    â€¢ Passwords, tokens, secrets
    â€¢ PII without masking
    â€¢ Excessive debug data in prod
    â€¢ Binary data
    â€¢ High-frequency loops
    
    Log Consistently:
    â€¢ Standard format across services
    â€¢ Consistent field names
    â€¢ Correlation IDs everywhere
    â€¢ Same timezone (UTC)
    
    Monitor Your Logs:
    â€¢ Log volume trends
    â€¢ Error rate alerts
    â€¢ Missing logs (gaps)
    â€¢ Storage utilization
    â€¢ Query performance
    
    Key Insight:
    Effective logging is essential for observability.
    Use structured logging, correlate events, and
    analyze patterns to maintain system health.
    """
    
    return {
        "messages": [AIMessage(content=f"ðŸ“Š Log Analyzer:\n{summary}")]
    }


# Build the graph
def build_logging_graph():
    """Build the logging pattern graph"""
    workflow = StateGraph(LoggingState)
    
    workflow.add_node("collector", log_collector)
    workflow.add_node("formatter", log_formatter)
    workflow.add_node("aggregator", log_aggregator)
    workflow.add_node("analyzer", log_analyzer)
    
    workflow.add_edge(START, "collector")
    workflow.add_edge("collector", "formatter")
    workflow.add_edge("formatter", "aggregator")
    workflow.add_edge("aggregator", "analyzer")
    workflow.add_edge("analyzer", END)
    
    return workflow.compile()


# Example usage
if __name__ == "__main__":
    graph = build_logging_graph()
    
    print("=== Logging MCP Pattern ===\n")
    
    # Test Case: Distributed system logging
    print("\n" + "="*70)
    print("TEST CASE: Multi-Service Log Aggregation")
    print("="*70)
    
    state = {
        "messages": [],
        "log_level": "DEBUG",
        "log_format": "JSON",
        "log_entries": [],
        "sources": ["app-server", "database", "cache", "message-queue"],
        "total_logs": 0,
        "error_count": 0,
        "warning_count": 0,
        "correlation_ids": {}
    }
    
    result = graph.invoke(state)
    
    for msg in result["messages"]:
        print(f"\n{msg.content}")
        print("-" * 70)
    
    print(f"\nTotal Logs Collected: {result.get('total_logs', 0)}")
    print(f"Errors: {result.get('error_count', 0)}")
    print(f"Warnings: {result.get('warning_count', 0)}")
    print(f"Unique Correlations: {len(result.get('correlation_ids', {}))}")
