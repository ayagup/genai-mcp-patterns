"""
Alerting MCP Pattern

This pattern implements intelligent alerting with notification routing,
escalation policies, and alert aggregation.

Key Features:
- Multi-channel alerting
- Smart alert routing
- Escalation management
- Alert deduplication
- Notification throttling
"""

from typing import TypedDict, Sequence, Annotated, List, Dict
import operator
import time
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langgraph.graph import StateGraph, START, END


# Define the state
class AlertingState(TypedDict):
    """State for alerting pattern"""
    messages: Annotated[Sequence[HumanMessage | SystemMessage | AIMessage], operator.add]
    alerts: List[Dict]  # [{id, severity, source, message, timestamp, status}]
    notification_channels: List[str]  # ["email", "slack", "pagerduty"]
    routing_rules: Dict[str, List[str]]  # {severity: [channels]}
    escalations: List[Dict]  # [{alert_id, level, assignee, timestamp}]
    suppressed_alerts: List[str]


# Initialize LLM
llm = ChatOpenAI(model="gpt-4", temperature=0)


# Alert Manager
def alert_manager(state: AlertingState) -> AlertingState:
    """Manages incoming alerts and deduplication"""
    alerts = state.get("alerts", [])
    
    system_message = SystemMessage(content="""You are an alert management system.
    Process, deduplicate, and route alerts appropriately.""")
    
    user_message = HumanMessage(content=f"""Manage alerts:

Current Alerts: {len(alerts) if alerts else 'None'}

Set up alert management.""")
    
    response = llm.invoke([system_message, user_message])
    
    # Generate sample alerts if not provided
    if not alerts:
        import uuid
        current_time = int(time.time())
        
        alerts = [
            {
                "id": str(uuid.uuid4())[:8],
                "severity": "critical",
                "source": "api-gateway",
                "message": "Service unavailable - health check failing",
                "timestamp": current_time,
                "status": "active",
                "metadata": {"endpoint": "/api/users", "error_rate": 85.5}
            },
            {
                "id": str(uuid.uuid4())[:8],
                "severity": "warning",
                "source": "database",
                "message": "Connection pool utilization high",
                "timestamp": current_time + 30,
                "status": "active",
                "metadata": {"pool_usage": 92, "max_connections": 100}
            },
            {
                "id": str(uuid.uuid4())[:8],
                "severity": "info",
                "source": "cache",
                "message": "Cache hit rate below threshold",
                "timestamp": current_time + 60,
                "status": "active",
                "metadata": {"hit_rate": 65.2, "threshold": 80}
            },
            {
                "id": str(uuid.uuid4())[:8],
                "severity": "critical",
                "source": "payment-service",
                "message": "Payment gateway timeout rate elevated",
                "timestamp": current_time + 90,
                "status": "active",
                "metadata": {"timeout_rate": 12.5, "sla_threshold": 1.0}
            }
        ]
    
    # Define notification channels and routing
    notification_channels = ["email", "slack", "pagerduty", "webhook"]
    routing_rules = {
        "critical": ["pagerduty", "slack", "email"],
        "warning": ["slack", "email"],
        "info": ["email"]
    }
    
    # Deduplicate alerts (suppress duplicates within 5 minutes)
    suppressed_alerts = []
    seen = set()
    for alert in alerts:
        alert_key = f"{alert['source']}-{alert['severity']}-{alert['message']}"
        if alert_key in seen:
            suppressed_alerts.append(alert["id"])
            alert["status"] = "suppressed"
        else:
            seen.add(alert_key)
    
    report = f"""
    ðŸš¨ Alert Management:
    
    Alert Overview:
    â€¢ Total Alerts: {len(alerts)}
    â€¢ Active: {sum(1 for a in alerts if a['status'] == 'active')}
    â€¢ Suppressed: {len(suppressed_alerts)}
    â€¢ Critical: {sum(1 for a in alerts if a['severity'] == 'critical')}
    â€¢ Warning: {sum(1 for a in alerts if a['severity'] == 'warning')}
    
    Alerting Concepts:
    
    Alert Severity Levels:
    
    Critical/P1:
    â€¢ Service down
    â€¢ Data loss
    â€¢ Security breach
    â€¢ Immediate response
    â€¢ Page on-call
    
    Warning/P2:
    â€¢ Degraded performance
    â€¢ Threshold breach
    â€¢ Approaching limits
    â€¢ Business hours response
    â€¢ Email/Slack notification
    
    Info/P3:
    â€¢ Normal events
    â€¢ Informational
    â€¢ No immediate action
    â€¢ Log and review
    â€¢ Batch notifications
    
    Alert States:
    
    Active:
    â€¢ Currently firing
    â€¢ Condition still met
    â€¢ Notifications sent
    â€¢ Awaiting acknowledgment
    
    Acknowledged:
    â€¢ Engineer notified
    â€¢ Work in progress
    â€¢ Still firing
    â€¢ No more notifications
    
    Resolved:
    â€¢ Condition cleared
    â€¢ Back to normal
    â€¢ Close incident
    â€¢ Post-mortem if needed
    
    Suppressed:
    â€¢ Intentionally muted
    â€¢ Maintenance window
    â€¢ Known issue
    â€¢ Temporary silence
    
    Notification Channels:
    
    Email:
    â€¢ Detailed information
    â€¢ Audit trail
    â€¢ Non-urgent
    â€¢ Batching possible
    â€¢ HTML formatting
    
    Slack/Teams:
    â€¢ Real-time notification
    â€¢ Team visibility
    â€¢ Quick discussion
    â€¢ Rich formatting
    â€¢ Emoji reactions
    
    PagerDuty/OpsGenie:
    â€¢ On-call rotation
    â€¢ Escalation policies
    â€¢ Acknowledgment tracking
    â€¢ Mobile push
    â€¢ Phone calls for P1
    
    Webhook:
    â€¢ Custom integrations
    â€¢ ITSM systems
    â€¢ Chatbots
    â€¢ Ticketing
    â€¢ Automation triggers
    
    Alert Routing Example (Prometheus):
    ```yaml
    route:
      receiver: 'default'
      group_by: ['alertname', 'cluster']
      group_wait: 10s
      group_interval: 5m
      repeat_interval: 12h
      
      routes:
      - match:
          severity: critical
        receiver: pagerduty
        continue: true
        
      - match:
          severity: warning
        receiver: slack
        
      - match_re:
          service: ^(payment|auth)
        receiver: critical-team
    
    receivers:
    - name: 'pagerduty'
      pagerduty_configs:
      - service_key: xxx
        
    - name: 'slack'
      slack_configs:
      - api_url: xxx
        channel: '#alerts'
    ```
    
    Deduplication Strategies:
    
    Time-based:
    â€¢ Suppress duplicates within window
    â€¢ Default: 5-15 minutes
    â€¢ Prevents alert storms
    
    Fingerprint:
    â€¢ Hash alert attributes
    â€¢ Match identical alerts
    â€¢ Group related events
    
    Correlation:
    â€¢ Root cause detection
    â€¢ Dependency awareness
    â€¢ Single notification for cascade
    
    Alert Aggregation:
    â€¢ Group by cluster
    â€¢ Batch notifications
    â€¢ Summary messages
    â€¢ Reduce noise
    
    Notification Formatting (Slack):
    ```python
    import requests
    
    def send_slack_alert(alert):
        payload = {{
            "attachments": [{{
                "color": "danger" if alert["severity"] == "critical" else "warning",
                "title": f"{{alert['severity'].upper()}}: {{alert['source']}}",
                "text": alert["message"],
                "fields": [
                    {{"title": "Severity", "value": alert["severity"], "short": True}},
                    {{"title": "Source", "value": alert["source"], "short": True}},
                    {{"title": "Time", "value": alert["timestamp"], "short": False}}
                ],
                "footer": "Alert System",
                "ts": alert["timestamp"]
            }}]
        }}
        
        requests.post(SLACK_WEBHOOK_URL, json=payload)
    ```
    """
    
    return {
        "messages": [AIMessage(content=f"ðŸš¨ Alert Manager:\n{response.content}\n{report}")],
        "alerts": alerts,
        "notification_channels": notification_channels,
        "routing_rules": routing_rules,
        "suppressed_alerts": suppressed_alerts
    }


# Escalation Manager
def escalation_manager(state: AlertingState) -> AlertingState:
    """Handles alert escalation based on policies"""
    alerts = state.get("alerts", [])
    routing_rules = state.get("routing_rules", {})
    
    system_message = SystemMessage(content="""You are an escalation management system.
    Handle alert escalation and ensure timely response.""")
    
    user_message = HumanMessage(content=f"""Manage escalations:

Active Alerts: {sum(1 for a in alerts if a.get('status') == 'active')}
Routing Rules: {len(routing_rules)}

Create escalation plan.""")
    
    response = llm.invoke([system_message, user_message])
    
    # Create escalations for critical alerts
    escalations = []
    current_time = int(time.time())
    
    for alert in alerts:
        if alert.get("status") == "active" and alert.get("severity") == "critical":
            escalations.append({
                "alert_id": alert["id"],
                "level": 1,
                "assignee": "on-call-engineer",
                "timestamp": current_time,
                "channels": routing_rules.get("critical", []),
                "timeout_minutes": 15
            })
    
    summary = f"""
    ðŸ“Š ALERTING COMPLETE
    
    Alerting Summary:
    â€¢ Total Alerts: {len(alerts)}
    â€¢ Active: {sum(1 for a in alerts if a.get('status') == 'active')}
    â€¢ Suppressed: {len(state.get('suppressed_alerts', []))}
    â€¢ Escalations: {len(escalations)}
    â€¢ Notification Channels: {len(state.get('notification_channels', []))}
    
    Escalations Created:
    {chr(10).join(f"â€¢ Alert {e['alert_id']}: Level {e['level']} â†’ {e['assignee']} via {', '.join(e['channels'])}" for e in escalations) if escalations else "â€¢ No escalations needed"}
    
    Alerting Pattern Process:
    1. Alert Manager â†’ Process and deduplicate alerts
    2. Escalation Manager â†’ Handle escalation policies
    
    Escalation Policies:
    
    Level 1 (0-15 min):
    â€¢ Notify on-call engineer
    â€¢ PagerDuty/OpsGenie
    â€¢ Phone + SMS + Push
    â€¢ Acknowledge required
    
    Level 2 (15-30 min):
    â€¢ Escalate to senior engineer
    â€¢ Manager notification
    â€¢ Team Slack channel
    â€¢ Incident commander assigned
    
    Level 3 (30+ min):
    â€¢ Escalate to team lead
    â€¢ Page multiple engineers
    â€¢ Incident response team
    â€¢ Status page update
    
    Level 4 (1+ hour):
    â€¢ CTO/VP Engineering
    â€¢ Cross-team coordination
    â€¢ Customer communication
    â€¢ Post-mortem scheduled
    
    Alert Fatigue Prevention:
    
    Meaningful Alerts:
    â€¢ Alert on symptoms, not causes
    â€¢ Action-oriented messages
    â€¢ Clear remediation steps
    â€¢ Remove noise
    
    Smart Throttling:
    â€¢ Rate limiting per alert
    â€¢ Exponential backoff
    â€¢ Quiet hours (optional)
    â€¢ Maintenance windows
    
    Alert Quality:
    â€¢ Review alert value
    â€¢ Track acknowledgment rate
    â€¢ Measure time to resolve
    â€¢ Remove unused alerts
    
    Runbook Integration:
    â€¢ Link to documentation
    â€¢ Auto-remediation steps
    â€¢ Common causes
    â€¢ Diagnostic commands
    
    Alert Testing:
    
    Synthetic Alerts:
    â€¢ Test notification flow
    â€¢ Verify routing
    â€¢ Check escalation
    â€¢ Practice response
    
    Chaos Engineering:
    â€¢ Trigger real failures
    â€¢ Verify alerts fire
    â€¢ Test recovery procedures
    â€¢ Validate runbooks
    
    Best Practices:
    
    Alert Design:
    â€¢ Actionable message
    â€¢ Clear severity
    â€¢ Context included
    â€¢ Runbook link
    â€¢ Related metrics
    
    On-Call Management:
    â€¢ Rotation schedule
    â€¢ Handoff process
    â€¢ Escalation chain
    â€¢ On-call compensation
    â€¢ Incident review
    
    Metrics to Track:
    â€¢ MTTA (Mean Time To Acknowledge)
    â€¢ MTTR (Mean Time To Resolve)
    â€¢ Alert volume trends
    â€¢ False positive rate
    â€¢ Escalation frequency
    
    Key Insight:
    Effective alerting balances comprehensive coverage
    with alert fatigue prevention through intelligent
    routing, deduplication, and escalation.
    """
    
    return {
        "messages": [AIMessage(content=f"âš¡ Escalation Manager:\n{response.content}\n{summary}")],
        "escalations": escalations
    }


# Build the graph
def build_alerting_graph():
    """Build the alerting pattern graph"""
    workflow = StateGraph(AlertingState)
    
    workflow.add_node("alert_manager", alert_manager)
    workflow.add_node("escalation_manager", escalation_manager)
    
    workflow.add_edge(START, "alert_manager")
    workflow.add_edge("alert_manager", "escalation_manager")
    workflow.add_edge("escalation_manager", END)
    
    return workflow.compile()


# Example usage
if __name__ == "__main__":
    graph = build_alerting_graph()
    
    print("=== Alerting MCP Pattern ===\n")
    
    # Test Case: Multi-severity alerting with escalation
    print("\n" + "="*70)
    print("TEST CASE: Alert Management and Escalation")
    print("="*70)
    
    state = {
        "messages": [],
        "alerts": [],
        "notification_channels": [],
        "routing_rules": {},
        "escalations": [],
        "suppressed_alerts": []
    }
    
    result = graph.invoke(state)
    
    for msg in result["messages"]:
        print(f"\n{msg.content}")
        print("-" * 70)
    
    print(f"\nAlerting Results:")
    print(f"Total Alerts: {len(result.get('alerts', []))}")
    print(f"Active: {sum(1 for a in result.get('alerts', []) if a.get('status') == 'active')}")
    print(f"Escalations: {len(result.get('escalations', []))}")
