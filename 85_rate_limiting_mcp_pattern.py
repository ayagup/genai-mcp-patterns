"""
Rate Limiting MCP Pattern

This pattern controls the rate of requests to prevent abuse, ensure fair usage,
and protect system resources through throttling and quota management.

Key Features:
- Request rate limiting
- Quota management
- Token bucket algorithm
- Sliding window counters
- User/IP-based limits
"""

from typing import TypedDict, Sequence, Annotated
import operator
import time
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langgraph.graph import StateGraph, START, END


# Define the state
class RateLimitState(TypedDict):
    """State for rate limiting pattern"""
    messages: Annotated[Sequence[HumanMessage | SystemMessage | AIMessage], operator.add]
    client_id: str
    request_count: int
    rate_limit: int  # requests per minute
    time_window: int  # seconds
    quota_limit: int  # total requests per day
    quota_used: int
    current_timestamp: float
    request_allowed: bool
    limit_exceeded: bool
    retry_after: int  # seconds
    limit_type: str  # "rate", "quota", "both"


# Initialize LLM
llm = ChatOpenAI(model="gpt-4", temperature=0)


# Rate Limit Checker
def rate_limit_checker(state: RateLimitState) -> RateLimitState:
    """Checks if request is within rate limits"""
    client_id = state.get("client_id", "")
    request_count = state.get("request_count", 0)
    rate_limit = state.get("rate_limit", 60)  # 60 req/min default
    time_window = state.get("time_window", 60)  # 60 seconds
    
    system_message = SystemMessage(content="""You are a rate limit checker. 
    Monitor request rates and enforce limits to prevent abuse.""")
    
    user_message = HumanMessage(content=f"""Check rate limit:

Client: {client_id}
Current Requests: {request_count}
Rate Limit: {rate_limit} requests per {time_window} seconds

Determine if request should be allowed.""")
    
    response = llm.invoke([system_message, user_message])
    
    # Check rate limit
    rate_exceeded = request_count >= rate_limit
    
    # Calculate retry-after time
    retry_after = time_window if rate_exceeded else 0
    
    rate_check_result = f"""
    üö¶ Rate Limit Check:
    
    ‚Ä¢ Client: {client_id}
    ‚Ä¢ Current Rate: {request_count}/{rate_limit} requests
    ‚Ä¢ Time Window: {time_window} seconds
    ‚Ä¢ Status: {'‚ùå EXCEEDED' if rate_exceeded else '‚úÖ WITHIN LIMIT'}
    ‚Ä¢ Utilization: {(request_count/rate_limit*100):.1f}%
    
    {'‚è±Ô∏è Retry After: ' + str(retry_after) + ' seconds' if rate_exceeded else '‚úÖ Request can proceed'}
    
    Rate Limit Algorithm: Token Bucket
    """
    
    return {
        "messages": [AIMessage(content=f"üö¶ Rate Limit Checker:\n{response.content}\n{rate_check_result}")],
        "request_allowed": not rate_exceeded,
        "limit_exceeded": rate_exceeded,
        "retry_after": retry_after
    }


# Quota Manager
def quota_manager(state: RateLimitState) -> RateLimitState:
    """Manages usage quotas"""
    client_id = state.get("client_id", "")
    quota_limit = state.get("quota_limit", 10000)  # 10k requests per day
    quota_used = state.get("quota_used", 0)
    request_allowed = state.get("request_allowed", False)
    
    system_message = SystemMessage(content="""You are a quota manager. 
    Track and enforce usage quotas to ensure fair resource allocation.""")
    
    user_message = HumanMessage(content=f"""Manage quota:

Client: {client_id}
Quota Used: {quota_used}/{quota_limit}
Request Allowed (by rate limit): {request_allowed}

Check if quota allows this request.""")
    
    response = llm.invoke([system_message, user_message])
    
    # Check quota
    quota_exceeded = quota_used >= quota_limit
    
    # Override rate limit decision if quota exceeded
    if quota_exceeded:
        request_allowed = False
    
    quota_percentage = (quota_used / quota_limit * 100) if quota_limit > 0 else 0
    
    # Determine warning thresholds
    if quota_percentage >= 90:
        quota_status = "üî¥ CRITICAL"
    elif quota_percentage >= 75:
        quota_status = "üü† WARNING"
    elif quota_percentage >= 50:
        quota_status = "üü° CAUTION"
    else:
        quota_status = "üü¢ NORMAL"
    
    quota_report = f"""
    üìä Quota Management:
    
    ‚Ä¢ Client: {client_id}
    ‚Ä¢ Quota Used: {quota_used:,}/{quota_limit:,} ({quota_percentage:.1f}%)
    ‚Ä¢ Status: {quota_status}
    ‚Ä¢ Request: {'‚ùå DENIED (quota exceeded)' if quota_exceeded else '‚úÖ Allowed'}
    
    Quota Limits:
    ‚Ä¢ Daily Limit: {quota_limit:,} requests
    ‚Ä¢ Remaining: {max(0, quota_limit - quota_used):,} requests
    ‚Ä¢ Reset Time: 24:00 UTC (in {24 - time.gmtime().tm_hour} hours)
    
    {'‚ö†Ô∏è Quota exhausted - upgrade plan required' if quota_exceeded else ''}
    """
    
    return {
        "messages": [AIMessage(content=f"üìä Quota Manager:\n{response.content}\n{quota_report}")],
        "request_allowed": request_allowed,
        "limit_exceeded": quota_exceeded or state.get("limit_exceeded", False)
    }


# Throttle Controller
def throttle_controller(state: RateLimitState) -> RateLimitState:
    """Controls request throttling"""
    client_id = state.get("client_id", "")
    request_allowed = state.get("request_allowed", False)
    limit_exceeded = state.get("limit_exceeded", False)
    retry_after = state.get("retry_after", 0)
    request_count = state.get("request_count", 0)
    rate_limit = state.get("rate_limit", 60)
    
    system_message = SystemMessage(content="""You are a throttle controller. 
    Implement throttling strategies to manage request flow.""")
    
    user_message = HumanMessage(content=f"""Control throttling:

Client: {client_id}
Request Allowed: {request_allowed}
Limit Exceeded: {limit_exceeded}
Current Load: {request_count}/{rate_limit}

Apply throttling strategy.""")
    
    response = llm.invoke([system_message, user_message])
    
    if limit_exceeded:
        throttle_action = "REJECT"
        http_status = 429  # Too Many Requests
        throttle_message = f"Rate limit exceeded. Retry after {retry_after} seconds"
    elif request_count / rate_limit > 0.8:  # 80% of limit
        throttle_action = "SLOW"
        http_status = 200
        throttle_message = "Request allowed but throttled (near limit)"
    else:
        throttle_action = "ALLOW"
        http_status = 200
        throttle_message = "Request allowed"
    
    throttle_report = f"""
    ‚ö° Throttle Control:
    
    ‚Ä¢ Action: {throttle_action}
    ‚Ä¢ HTTP Status: {http_status}
    ‚Ä¢ Message: {throttle_message}
    
    Throttling Strategy:
    ‚Ä¢ Algorithm: Token Bucket + Sliding Window
    ‚Ä¢ Burst Handling: Allow short bursts within limits
    ‚Ä¢ Fair Queuing: Prevent single client monopolization
    
    Response Headers:
    ‚Ä¢ X-RateLimit-Limit: {rate_limit}
    ‚Ä¢ X-RateLimit-Remaining: {max(0, rate_limit - request_count)}
    ‚Ä¢ X-RateLimit-Reset: {int(time.time()) + retry_after}
    {'‚Ä¢ Retry-After: ' + str(retry_after) if limit_exceeded else ''}
    
    {'‚ùå Request throttled' if limit_exceeded else '‚úÖ Request proceeding'}
    """
    
    return {
        "messages": [AIMessage(content=f"‚ö° Throttle Controller:\n{response.content}\n{throttle_report}")]
    }


# Rate Limit Monitor
def rate_limit_monitor(state: RateLimitState) -> RateLimitState:
    """Monitors rate limiting metrics and patterns"""
    client_id = state.get("client_id", "")
    request_count = state.get("request_count", 0)
    rate_limit = state.get("rate_limit", 60)
    quota_used = state.get("quota_used", 0)
    quota_limit = state.get("quota_limit", 10000)
    request_allowed = state.get("request_allowed", False)
    limit_exceeded = state.get("limit_exceeded", False)
    retry_after = state.get("retry_after", 0)
    
    summary = f"""
    üìà RATE LIMITING COMPLETE
    
    Client Information:
    ‚Ä¢ Client ID: {client_id}
    ‚Ä¢ Request Status: {'‚úÖ ALLOWED' if request_allowed else '‚ùå DENIED'}
    ‚Ä¢ Limit Exceeded: {'Yes ‚ùå' if limit_exceeded else 'No ‚úÖ'}
    
    Rate Limiting:
    ‚Ä¢ Current Rate: {request_count}/{rate_limit} per minute
    ‚Ä¢ Utilization: {(request_count/rate_limit*100):.1f}%
    ‚Ä¢ Remaining: {max(0, rate_limit - request_count)} requests
    
    Quota Management:
    ‚Ä¢ Daily Usage: {quota_used:,}/{quota_limit:,}
    ‚Ä¢ Utilization: {(quota_used/quota_limit*100):.1f}%
    ‚Ä¢ Remaining: {max(0, quota_limit - quota_used):,} requests
    
    {'Retry Information:' if limit_exceeded else ''}
    {f'‚Ä¢ Retry After: {retry_after} seconds' if limit_exceeded else ''}
    
    Rate Limiting Pattern Process:
    1. Rate Limit Check ‚Üí Verify request rate
    2. Quota Management ‚Üí Check daily quota
    3. Throttle Control ‚Üí Apply throttling
    4. Response ‚Üí Allow or deny request
    5. Monitor ‚Üí Track patterns and metrics
    
    Rate Limiting Algorithms:
    
    1. Token Bucket:
       ‚Ä¢ Tokens added at fixed rate
       ‚Ä¢ Each request consumes token
       ‚Ä¢ Allows bursts (bucket size)
       ‚Ä¢ Most common algorithm
    
    2. Leaky Bucket:
       ‚Ä¢ Requests processed at fixed rate
       ‚Ä¢ Queue overflows rejected
       ‚Ä¢ Smooth output rate
       ‚Ä¢ No bursts allowed
    
    3. Fixed Window:
       ‚Ä¢ Count requests in time window
       ‚Ä¢ Reset at window boundary
       ‚Ä¢ Simple but edge case issues
       ‚Ä¢ Can allow 2x burst at boundary
    
    4. Sliding Window:
       ‚Ä¢ Rolling time window
       ‚Ä¢ More accurate than fixed
       ‚Ä¢ Prevents boundary bursts
       ‚Ä¢ Higher complexity
    
    5. Sliding Log:
       ‚Ä¢ Track all request timestamps
       ‚Ä¢ Most accurate
       ‚Ä¢ Memory intensive
       ‚Ä¢ Not scalable
    
    Rate Limit Strategies:
    
    Per-User Limits:
    ‚Ä¢ Authenticated users
    ‚Ä¢ User-specific quotas
    ‚Ä¢ Tier-based limits
    ‚Ä¢ Fair usage policy
    
    Per-IP Limits:
    ‚Ä¢ Anonymous requests
    ‚Ä¢ DDoS prevention
    ‚Ä¢ Geographic limits
    ‚Ä¢ Shared IP considerations
    
    Per-API-Key Limits:
    ‚Ä¢ Application-based
    ‚Ä¢ Service tier enforcement
    ‚Ä¢ Easy tracking
    ‚Ä¢ Key rotation support
    
    Global Limits:
    ‚Ä¢ System-wide protection
    ‚Ä¢ Resource capacity limits
    ‚Ä¢ Prevent overload
    ‚Ä¢ Emergency throttling
    
    Common Rate Limit Tiers:
    
    Free Tier:
    ‚Ä¢ 60 requests/minute
    ‚Ä¢ 1,000 requests/day
    ‚Ä¢ Basic features only
    
    Basic Tier:
    ‚Ä¢ 300 requests/minute
    ‚Ä¢ 10,000 requests/day
    ‚Ä¢ Standard features
    
    Pro Tier:
    ‚Ä¢ 1,000 requests/minute
    ‚Ä¢ 100,000 requests/day
    ‚Ä¢ Advanced features
    
    Enterprise:
    ‚Ä¢ Custom limits
    ‚Ä¢ Dedicated resources
    ‚Ä¢ Priority support
    ‚Ä¢ SLA guarantees
    
    HTTP Status Codes:
    ‚Ä¢ 200: Request allowed
    ‚Ä¢ 429: Too Many Requests
    ‚Ä¢ 503: Service temporarily unavailable
    
    Response Headers:
    ‚Ä¢ X-RateLimit-Limit: Max requests
    ‚Ä¢ X-RateLimit-Remaining: Requests left
    ‚Ä¢ X-RateLimit-Reset: Reset timestamp
    ‚Ä¢ Retry-After: Seconds to wait
    
    Rate Limiting Best Practices:
    ‚Ä¢ Clear limit documentation
    ‚Ä¢ Informative error messages
    ‚Ä¢ Retry-After headers
    ‚Ä¢ Gradual limit increases
    ‚Ä¢ Burst tolerance
    ‚Ä¢ Multiple limit types
    ‚Ä¢ Real-time monitoring
    ‚Ä¢ Alert on anomalies
    ‚Ä¢ Dynamic adjustment
    ‚Ä¢ Whitelist critical clients
    
    Implementation Considerations:
    ‚Ä¢ Distributed systems (Redis)
    ‚Ä¢ Atomic operations
    ‚Ä¢ Clock synchronization
    ‚Ä¢ Storage efficiency
    ‚Ä¢ Performance impact
    ‚Ä¢ Failover handling
    
    Bypass Mechanisms:
    ‚Ä¢ Whitelist trusted IPs
    ‚Ä¢ Admin override
    ‚Ä¢ Emergency access
    ‚Ä¢ Rate limit exemptions
    
    Common Use Cases:
    ‚Ä¢ API rate limiting
    ‚Ä¢ Login attempt limits
    ‚Ä¢ Search query limits
    ‚Ä¢ Email sending limits
    ‚Ä¢ File upload limits
    ‚Ä¢ Comment posting limits
    ‚Ä¢ Payment processing limits
    
    Abuse Prevention:
    ‚Ä¢ DDoS protection
    ‚Ä¢ Brute force prevention
    ‚Ä¢ Scraping prevention
    ‚Ä¢ Spam prevention
    ‚Ä¢ Resource exhaustion protection
    
    Key Insight:
    Rate limiting controls request rates to prevent abuse, ensure
    fair resource allocation, and protect system stability. Essential
    for APIs, web services, and any multi-user system.
    """
    
    return {
        "messages": [AIMessage(content=f"üìä Rate Limit Monitor:\n{summary}")]
    }


# Build the graph
def build_rate_limit_graph():
    """Build the rate limiting pattern graph"""
    workflow = StateGraph(RateLimitState)
    
    workflow.add_node("rate_checker", rate_limit_checker)
    workflow.add_node("quota_mgr", quota_manager)
    workflow.add_node("throttle", throttle_controller)
    workflow.add_node("monitor", rate_limit_monitor)
    
    workflow.add_edge(START, "rate_checker")
    workflow.add_edge("rate_checker", "quota_mgr")
    workflow.add_edge("quota_mgr", "throttle")
    workflow.add_edge("throttle", "monitor")
    workflow.add_edge("monitor", END)
    
    return workflow.compile()


# Example usage
if __name__ == "__main__":
    graph = build_rate_limit_graph()
    
    print("=== Rate Limiting MCP Pattern ===\n")
    
    # Test Case 1: Within limits
    print("\n" + "="*70)
    print("TEST CASE 1: Request Within Limits")
    print("="*70)
    
    state1 = {
        "messages": [],
        "client_id": "user_12345",
        "request_count": 45,  # Below 60/min limit
        "rate_limit": 60,
        "time_window": 60,
        "quota_limit": 10000,
        "quota_used": 5000,  # 50% of daily quota
        "current_timestamp": time.time(),
        "request_allowed": False,
        "limit_exceeded": False,
        "retry_after": 0,
        "limit_type": "both"
    }
    
    result1 = graph.invoke(state1)
    
    for msg in result1["messages"]:
        print(f"\n{msg.content}")
        print("-" * 70)
    
    print(f"\nRequest Status: {'‚úÖ ALLOWED' if result1.get('request_allowed') else '‚ùå DENIED'}")
    
    # Test Case 2: Rate limit exceeded
    print("\n\n" + "="*70)
    print("TEST CASE 2: Rate Limit Exceeded")
    print("="*70)
    
    state2 = {
        "messages": [],
        "client_id": "user_67890",
        "request_count": 65,  # Exceeds 60/min limit
        "rate_limit": 60,
        "time_window": 60,
        "quota_limit": 10000,
        "quota_used": 2000,
        "current_timestamp": time.time(),
        "request_allowed": False,
        "limit_exceeded": False,
        "retry_after": 0,
        "limit_type": "both"
    }
    
    result2 = graph.invoke(state2)
    
    print(f"\nClient: {state2['client_id']}")
    print(f"Request Rate: {state2['request_count']}/{state2['rate_limit']}")
    print(f"Status: {'DENIED ‚ùå' if result2.get('limit_exceeded') else 'ALLOWED ‚úÖ'}")
    print(f"Retry After: {result2.get('retry_after', 0)} seconds")
    
    # Test Case 3: Quota exceeded
    print("\n\n" + "="*70)
    print("TEST CASE 3: Daily Quota Exceeded")
    print("="*70)
    
    state3 = {
        "messages": [],
        "client_id": "user_99999",
        "request_count": 30,  # Within rate limit
        "rate_limit": 60,
        "time_window": 60,
        "quota_limit": 10000,
        "quota_used": 10005,  # Exceeded daily quota
        "current_timestamp": time.time(),
        "request_allowed": False,
        "limit_exceeded": False,
        "retry_after": 0,
        "limit_type": "both"
    }
    
    result3 = graph.invoke(state3)
    
    print(f"\nClient: {state3['client_id']}")
    print(f"Daily Quota: {state3['quota_used']:,}/{state3['quota_limit']:,}")
    print(f"Status: {'DENIED ‚ùå (Quota Exceeded)' if result3.get('limit_exceeded') else 'ALLOWED ‚úÖ'}")
