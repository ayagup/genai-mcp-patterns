"""
Garbage Collection MCP Pattern

This pattern automatically identifies and reclaims unused resources,
preventing memory leaks and optimizing resource utilization.

Key Features:
- Automatic resource detection
- Reference tracking
- Memory reclamation
- Leak detection and prevention
- Performance optimization
"""

from typing import TypedDict, Sequence, Annotated, List, Dict
import operator
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langgraph.graph import StateGraph, START, END


# Define the state
class GarbageCollectionState(TypedDict):
    """State for garbage collection pattern"""
    messages: Annotated[Sequence[HumanMessage | SystemMessage | AIMessage], operator.add]
    gc_strategy: str  # "reference_counting", "mark_and_sweep", "generational", "concurrent"
    total_objects: int
    live_objects: int
    garbage_objects: int
    memory_before_mb: float
    memory_after_mb: float
    gc_cycles: int
    collection_time_ms: float


# Initialize LLM
llm = ChatOpenAI(model="gpt-4", temperature=0)


# Garbage Collector
def garbage_collector(state: GarbageCollectionState) -> GarbageCollectionState:
    """Performs garbage collection"""
    gc_strategy = state.get("gc_strategy", "mark_and_sweep")
    total_objects = state.get("total_objects", 10000)
    
    system_message = SystemMessage(content="""You are a garbage collector.
    Identify and reclaim unused resources automatically.""")
    
    user_message = HumanMessage(content=f"""Perform garbage collection:

Strategy: {gc_strategy}
Total Objects: {total_objects}

Run garbage collection cycle.""")
    
    response = llm.invoke([system_message, user_message])
    
    # Simulate garbage collection
    import random
    garbage_objects = int(total_objects * 0.25)  # 25% garbage
    live_objects = total_objects - garbage_objects
    memory_before_mb = total_objects * 0.01  # 10KB per object
    memory_after_mb = live_objects * 0.01
    collection_time_ms = random.uniform(10, 50)
    gc_cycles = state.get("gc_cycles", 0) + 1
    
    report = f"""
    ðŸ—‘ï¸ Garbage Collection:
    
    Collection Results:
    â€¢ Strategy: {gc_strategy.upper()}
    â€¢ Objects Scanned: {total_objects:,}
    â€¢ Live Objects: {live_objects:,}
    â€¢ Garbage Collected: {garbage_objects:,}
    â€¢ Memory Before: {memory_before_mb:.2f} MB
    â€¢ Memory After: {memory_after_mb:.2f} MB
    â€¢ Memory Freed: {memory_before_mb - memory_after_mb:.2f} MB
    â€¢ Collection Time: {collection_time_ms:.2f} ms
    â€¢ GC Cycles: {gc_cycles}
    
    Garbage Collection Strategies:
    
    1. Reference Counting:
       â€¢ Track reference count per object
       â€¢ Deallocate when count = 0
       â€¢ Immediate collection
       â€¢ Cannot handle cycles
       â€¢ Example: Python (partial), Swift
    
    2. Mark and Sweep:
       â€¢ Mark: Trace from roots
       â€¢ Sweep: Collect unmarked
       â€¢ Handles cycles
       â€¢ Stop-the-world pauses
       â€¢ Example: Java, Go
    
    3. Generational:
       â€¢ Young generation (frequent)
       â€¢ Old generation (infrequent)
       â€¢ Most objects die young
       â€¢ Optimized for common case
       â€¢ Example: Java G1GC, Python
    
    4. Concurrent:
       â€¢ Run alongside application
       â€¢ Reduce pause times
       â€¢ More complex
       â€¢ Higher throughput
       â€¢ Example: Go, Java CMS
    
    5. Incremental:
       â€¢ Break collection into steps
       â€¢ Interleave with application
       â€¢ Bounded pause times
       â€¢ More overhead
       â€¢ Example: V8 (JavaScript)
    
    GC Algorithms in Detail:
    
    Mark-and-Sweep:
    ```python
    def mark_and_sweep():
        # Mark phase
        marked = set()
        stack = [root for root in gc_roots]
        
        while stack:
            obj = stack.pop()
            if obj not in marked:
                marked.add(obj)
                stack.extend(obj.references)
        
        # Sweep phase
        for obj in all_objects:
            if obj not in marked:
                deallocate(obj)
    ```
    
    Generational:
    ```python
    class GenerationalGC:
        def __init__(self):
            self.young = []  # Recently allocated
            self.old = []    # Survived multiple GCs
        
        def collect_young(self):
            # Minor GC - frequent, fast
            survivors = []
            for obj in self.young:
                if is_reachable(obj):
                    obj.age += 1
                    if obj.age > THRESHOLD:
                        self.old.append(obj)
                    else:
                        survivors.append(obj)
            self.young = survivors
        
        def collect_old(self):
            # Major GC - infrequent, slow
            # Full mark-and-sweep
            pass
    ```
    
    Reference Counting:
    ```python
    class RefCounted:
        def __init__(self):
            self.ref_count = 0
            self.data = None
        
        def incref(self):
            self.ref_count += 1
        
        def decref(self):
            self.ref_count -= 1
            if self.ref_count == 0:
                self.deallocate()
        
        def deallocate(self):
            # Free resources
            del self.data
    ```
    
    Language-Specific GC:
    
    Java:
    â€¢ Serial GC: Single-threaded
    â€¢ Parallel GC: Multi-threaded
    â€¢ G1GC: Generational, region-based
    â€¢ ZGC: Low-latency, concurrent
    â€¢ Shenandoah: Ultra-low pause
    
    Python:
    â€¢ Reference counting (primary)
    â€¢ Cycle detector (backup)
    â€¢ Generational (3 generations)
    â€¢ Manual control: gc.collect()
    
    Go:
    â€¢ Concurrent mark-sweep
    â€¢ Tri-color marking
    â€¢ Write barriers
    â€¢ Sub-millisecond pauses
    â€¢ Automatic tuning
    
    JavaScript (V8):
    â€¢ Generational (Scavenge, Mark-Compact)
    â€¢ Incremental marking
    â€¢ Concurrent sweeping
    â€¢ Parallel compaction
    â€¢ Orinoco optimizer
    
    .NET:
    â€¢ Generational (Gen 0, 1, 2)
    â€¢ Server vs Workstation GC
    â€¢ Background GC
    â€¢ Large Object Heap
    â€¢ Pinned objects
    
    GC Roots (Starting Points):
    
    Common Roots:
    â€¢ Stack variables
    â€¢ Static fields
    â€¢ CPU registers
    â€¢ JNI references
    â€¢ Thread locals
    â€¢ Finalizer queue
    
    GC Tuning:
    
    JVM Flags:
    ```bash
    # Use G1GC
    -XX:+UseG1GC
    
    # Set heap size
    -Xms2g -Xmx4g
    
    # GC logging
    -Xlog:gc*:file=gc.log
    
    # Pause time goal
    -XX:MaxGCPauseMillis=200
    
    # Young generation size
    -XX:NewRatio=2
    ```
    
    Go GC Tuning:
    ```bash
    # Set GC target percentage
    GOGC=100  # Default: GC when heap doubles
    
    # Debug GC
    GODEBUG=gctrace=1
    
    # Soft memory limit
    GOMEMLIMIT=4GiB
    ```
    
    Python GC Control:
    ```python
    import gc
    
    # Disable automatic GC
    gc.disable()
    
    # Manual collection
    gc.collect()
    
    # Tune thresholds
    gc.set_threshold(700, 10, 10)
    
    # Get stats
    print(gc.get_stats())
    ```
    """
    
    return {
        "messages": [AIMessage(content=f"ðŸ—‘ï¸ Garbage Collector:\n{response.content}\n{report}")],
        "live_objects": live_objects,
        "garbage_objects": garbage_objects,
        "memory_after_mb": memory_after_mb,
        "collection_time_ms": collection_time_ms,
        "gc_cycles": gc_cycles
    }


# Leak Detector
def leak_detector(state: GarbageCollectionState) -> GarbageCollectionState:
    """Detects memory leaks and resource issues"""
    gc_cycles = state.get("gc_cycles", 0)
    memory_after_mb = state.get("memory_after_mb", 0.0)
    
    system_message = SystemMessage(content="""You are a memory leak detector.
    Identify potential memory leaks and resource issues.""")
    
    user_message = HumanMessage(content=f"""Detect memory leaks:

GC Cycles: {gc_cycles}
Current Memory: {memory_after_mb:.2f} MB

Analyze for leaks.""")
    
    response = llm.invoke([system_message, user_message])
    
    # Simulate leak detection
    leak_detected = memory_after_mb > 90.0  # Threshold
    
    report = f"""
    ðŸ” Leak Detection:
    
    Analysis Results:
    â€¢ GC Cycles Run: {gc_cycles}
    â€¢ Current Memory: {memory_after_mb:.2f} MB
    â€¢ Leak Detected: {'Yes âš ï¸' if leak_detected else 'No âœ…'}
    
    Common Memory Leak Causes:
    
    1. Forgotten References:
       â€¢ Event listeners not removed
       â€¢ Callbacks not cleared
       â€¢ Cache entries never expire
       â€¢ Global variables accumulate
    
    2. Circular References:
       â€¢ Objects reference each other
       â€¢ Parent-child cycles
       â€¢ Closure captures
       â€¢ Not handled by ref counting
    
    3. Resource Leaks:
       â€¢ Unclosed files
       â€¢ Unreleased connections
       â€¢ Unfreed memory allocations
       â€¢ Thread leaks
    
    4. Framework-Specific:
       â€¢ React: Unmounted components
       â€¢ Android: Context references
       â€¢ iOS: Retain cycles
       â€¢ Node.js: Event emitters
    
    Leak Detection Tools:
    
    Java:
    â€¢ VisualVM
    â€¢ Java Mission Control
    â€¢ Eclipse MAT (Memory Analyzer)
    â€¢ YourKit Java Profiler
    â€¢ JProfiler
    
    Python:
    â€¢ tracemalloc
    â€¢ memory_profiler
    â€¢ objgraph
    â€¢ pympler
    â€¢ guppy3
    
    JavaScript:
    â€¢ Chrome DevTools Heap Profiler
    â€¢ Memory snapshots
    â€¢ Allocation timeline
    â€¢ Retaining path analysis
    
    .NET:
    â€¢ dotMemory
    â€¢ PerfView
    â€¢ Debug Diagnostic Tool
    â€¢ CLR Profiler
    
    Detection Techniques:
    
    Heap Dump Analysis:
    ```python
    # Python
    import tracemalloc
    
    tracemalloc.start()
    
    # ... run code ...
    
    snapshot = tracemalloc.take_snapshot()
    top_stats = snapshot.statistics('lineno')
    
    for stat in top_stats[:10]:
        print(stat)
    ```
    
    Object Tracking:
    ```python
    import gc
    import sys
    
    def find_leaks(obj_type):
        objects = gc.get_objects()
        return [obj for obj in objects 
                if isinstance(obj, obj_type)]
    
    # Find all dict objects
    dicts = find_leaks(dict)
    print(f"Dict count: {{len(dicts)}}")
    ```
    
    Reference Counting:
    ```python
    import sys
    
    obj = {{'data': 'value'}}
    print(sys.getrefcount(obj))  # 2
    
    ref = obj
    print(sys.getrefcount(obj))  # 3
    
    del ref
    print(sys.getrefcount(obj))  # 2
    ```
    
    Memory Profiling:
    ```python
    from memory_profiler import profile
    
    @profile
    def my_function():
        large_list = [i for i in range(1000000)]
        return sum(large_list)
    ```
    
    Prevention Strategies:
    
    1. RAII Pattern (C++):
       â€¢ Resource Acquisition Is Initialization
       â€¢ Automatic cleanup in destructor
       â€¢ Smart pointers (unique_ptr, shared_ptr)
    
    2. Context Managers (Python):
       ```python
       with open('file.txt') as f:
           data = f.read()
       # File automatically closed
       ```
    
    3. Try-Finally:
       ```python
       resource = acquire()
       try:
           use(resource)
       finally:
           release(resource)
       ```
    
    4. Weak References:
       ```python
       import weakref
       
       obj = MyClass()
       weak_ref = weakref.ref(obj)
       
       # Won't prevent GC
       obj = None  # Object can be collected
       ```
    
    5. Event Cleanup:
       ```javascript
       // Remove event listeners
       element.removeEventListener('click', handler);
       
       // React useEffect cleanup
       useEffect(() => {{
         const timer = setInterval(...);
         return () => clearInterval(timer);
       }}, []);
       ```
    
    Monitoring Metrics:
    
    Key Indicators:
    â€¢ Memory growth over time
    â€¢ GC frequency increasing
    â€¢ GC pause times growing
    â€¢ Heap size not shrinking
    â€¢ Object count increasing
    
    Alerting Thresholds:
    â€¢ Memory > 80% for 10+ minutes
    â€¢ GC pause > 1 second
    â€¢ GC every < 10 seconds
    â€¢ Heap growth > 10% per hour
    â€¢ Old gen usage > 90%
    """
    
    return {
        "messages": [AIMessage(content=f"ðŸ” Leak Detector:\n{response.content}\n{report}")]
    }


# GC Monitor
def gc_monitor(state: GarbageCollectionState) -> GarbageCollectionState:
    """Monitors garbage collection performance"""
    gc_strategy = state.get("gc_strategy", "")
    total_objects = state.get("total_objects", 0)
    live_objects = state.get("live_objects", 0)
    garbage_objects = state.get("garbage_objects", 0)
    memory_before_mb = state.get("memory_before_mb", 0.0)
    memory_after_mb = state.get("memory_after_mb", 0.0)
    collection_time_ms = state.get("collection_time_ms", 0.0)
    gc_cycles = state.get("gc_cycles", 0)
    
    collection_efficiency = (garbage_objects / total_objects * 100) if total_objects > 0 else 0
    memory_freed = memory_before_mb - memory_after_mb
    
    summary = f"""
    ðŸ“Š GARBAGE COLLECTION COMPLETE
    
    GC Summary:
    â€¢ Strategy: {gc_strategy.upper()}
    â€¢ Total Objects: {total_objects:,}
    â€¢ Live Objects: {live_objects:,}
    â€¢ Garbage Collected: {garbage_objects:,}
    â€¢ Collection Efficiency: {collection_efficiency:.1f}%
    â€¢ Memory Freed: {memory_freed:.2f} MB
    â€¢ Collection Time: {collection_time_ms:.2f} ms
    â€¢ GC Cycles: {gc_cycles}
    
    Garbage Collection Pattern Process:
    1. Garbage Collector â†’ Reclaim unused resources
    2. Leak Detector â†’ Identify memory leaks
    3. Monitor â†’ Track GC performance
    
    GC Performance Metrics:
    
    Throughput:
    â€¢ Application time / Total time
    â€¢ Higher is better
    â€¢ 95%+ is good
    â€¢ Trade-off with latency
    
    Latency:
    â€¢ GC pause duration
    â€¢ Lower is better
    â€¢ < 100ms for interactive apps
    â€¢ < 10ms for real-time systems
    
    Memory Overhead:
    â€¢ Heap size / Live data
    â€¢ Lower is better
    â€¢ 1.5-2x is typical
    â€¢ Depends on GC algorithm
    
    Best Practices:
    
    Application Design:
    â€¢ Minimize object allocation
    â€¢ Reuse objects (pools)
    â€¢ Use primitives when possible
    â€¢ Avoid finalizers
    â€¢ Clear references explicitly
    
    GC Configuration:
    â€¢ Choose appropriate GC
    â€¢ Set heap size correctly
    â€¢ Tune generation sizes
    â€¢ Monitor and adjust
    â€¢ Profile before optimizing
    
    Code Patterns:
    â€¢ Use try-with-resources
    â€¢ Implement AutoCloseable
    â€¢ Avoid large objects
    â€¢ Stream large datasets
    â€¢ Batch operations
    
    Monitoring:
    â€¢ Track GC frequency
    â€¢ Monitor pause times
    â€¢ Watch heap usage
    â€¢ Alert on anomalies
    â€¢ Analyze GC logs
    
    Real-World Examples:
    
    Twitter:
    â€¢ Custom GC tuning for JVM
    â€¢ Reduced GC pause times
    â€¢ Improved throughput
    â€¢ Better user experience
    
    Netflix:
    â€¢ Microservices with tuned GC
    â€¢ G1GC for most services
    â€¢ ZGC for ultra-low latency
    â€¢ Continuous monitoring
    
    Google:
    â€¢ Custom GC in Go
    â€¢ Concurrent mark-sweep
    â€¢ Sub-millisecond pauses
    â€¢ Billions of objects/sec
    
    GC Trade-offs:
    
    Throughput vs Latency:
    â€¢ Throughput GC: Longer pauses, higher throughput
    â€¢ Low-latency GC: Shorter pauses, lower throughput
    â€¢ Choose based on requirements
    
    Memory vs CPU:
    â€¢ More memory â†’ Less GC
    â€¢ More CPU â†’ Faster GC
    â€¢ Balance based on resources
    
    Simplicity vs Performance:
    â€¢ Simple GC: Easy to tune
    â€¢ Complex GC: Better performance
    â€¢ Consider maintenance cost
    
    When to Optimize GC:
    
    Symptoms:
    â€¢ Frequent out-of-memory errors
    â€¢ Long GC pause times
    â€¢ High GC overhead (> 10%)
    â€¢ Memory leaks detected
    â€¢ Poor application performance
    
    Actions:
    â€¢ Profile heap usage
    â€¢ Analyze GC logs
    â€¢ Tune GC parameters
    â€¢ Fix memory leaks
    â€¢ Optimize code
    â€¢ Consider GC-less regions
    
    Alternative Approaches:
    
    Manual Memory Management:
    â€¢ C/C++: malloc/free
    â€¢ Rust: Ownership system
    â€¢ Full control
    â€¢ No GC pauses
    â€¢ Higher complexity
    
    Arena Allocation:
    â€¢ Allocate in arena
    â€¢ Free entire arena at once
    â€¢ Fast allocation
    â€¢ No per-object tracking
    â€¢ Good for temporary data
    
    Stack Allocation:
    â€¢ Automatic cleanup
    â€¢ No GC needed
    â€¢ Very fast
    â€¢ Limited lifetime
    
    Key Insight:
    Garbage collection automates memory management but
    requires understanding and tuning for optimal
    performance. Monitor GC metrics and optimize when needed.
    """
    
    return {
        "messages": [AIMessage(content=f"ðŸ“Š GC Monitor:\n{summary}")]
    }


# Build the graph
def build_gc_graph():
    """Build the garbage collection pattern graph"""
    workflow = StateGraph(GarbageCollectionState)
    
    workflow.add_node("collector", garbage_collector)
    workflow.add_node("leak_detector", leak_detector)
    workflow.add_node("monitor", gc_monitor)
    
    workflow.add_edge(START, "collector")
    workflow.add_edge("collector", "leak_detector")
    workflow.add_edge("leak_detector", "monitor")
    workflow.add_edge("monitor", END)
    
    return workflow.compile()


# Example usage
if __name__ == "__main__":
    graph = build_gc_graph()
    
    print("=== Garbage Collection MCP Pattern ===\n")
    
    # Test Case: Mark-and-sweep garbage collection
    print("\n" + "="*70)
    print("TEST CASE: Automatic Garbage Collection")
    print("="*70)
    
    state = {
        "messages": [],
        "gc_strategy": "mark_and_sweep",
        "total_objects": 10000,
        "live_objects": 0,
        "garbage_objects": 0,
        "memory_before_mb": 100.0,
        "memory_after_mb": 0.0,
        "gc_cycles": 0,
        "collection_time_ms": 0.0
    }
    
    result = graph.invoke(state)
    
    for msg in result["messages"]:
        print(f"\n{msg.content}")
        print("-" * 70)
    
    print(f"\nGarbage Collected: {result.get('garbage_objects', 0):,} objects")
    print(f"Memory Freed: {result.get('memory_before_mb', 0) - result.get('memory_after_mb', 0):.2f} MB")
    print(f"Collection Time: {result.get('collection_time_ms', 0):.2f} ms")
    print(f"\nðŸŽ‰ RESOURCE MANAGEMENT PATTERNS COMPLETE! (Patterns 91-100)")
